digraph {
	graph [size="642.9,642.9"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139952119481664 [label="
 (4, 8, 384, 512)" fillcolor=darkolivegreen1]
	139951966990592 [label=ConvolutionBackward0]
	139951966990688 -> 139951966990592
	139951966990688 [label=ReluBackward0]
	139951966990736 -> 139951966990688
	139951966990736 [label=NativeGroupNormBackward0]
	139951966990832 -> 139951966990736
	139951966990832 [label=ReluBackward0]
	139951966991024 -> 139951966990832
	139951966991024 [label=ConvolutionBackward0]
	139951966991120 -> 139951966991024
	139951966991120 [label=ReluBackward0]
	139951966991312 -> 139951966991120
	139951966991312 [label=ConvolutionBackward0]
	139951966991408 -> 139951966991312
	139951966991408 [label=UpsampleBilinear2DBackward0]
	139951966991600 -> 139951966991408
	139951966991600 [label=AddBackward0]
	139951966991696 -> 139951966991600
	139951966991696 [label=ReluBackward0]
	139951966991840 -> 139951966991696
	139951966991840 [label=ConvolutionBackward0]
	139951966991936 -> 139951966991840
	139951966991936 [label=ReluBackward0]
	139951966992080 -> 139951966991936
	139951966992080 [label=ConvolutionBackward0]
	139951966992176 -> 139951966992080
	139951966992176 [label=CatBackward0]
	139951966992320 -> 139951966992176
	139951966992320 [label=ReluBackward0]
	139951966992464 -> 139951966992320
	139951966992464 [label=NativeGroupNormBackward0]
	139951966992560 -> 139951966992464
	139951966992560 [label=ConvolutionBackward0]
	139951966992752 -> 139951966992560
	139951966992752 [label=MulBackward0]
	139951966992944 -> 139951966992752
	139951966992944 [label=CatBackward0]
	139951966993088 -> 139951966992944
	139951966993088 [label=ReluBackward0]
	139951966993280 -> 139951966993088
	139951966993280 [label=NativeGroupNormBackward0]
	139951966993376 -> 139951966993280
	139951966993376 [label=ConvolutionBackward0]
	139951966993568 -> 139951966993376
	139952177143296 [label="Expert_Gate.expert_layers.0.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	139952177143296 -> 139951966993568
	139951966993568 [label=AccumulateGrad]
	139951966993328 -> 139951966993280
	139952177142976 [label="Expert_Gate.expert_layers.0.bn1.weight
 (64)" fillcolor=lightblue]
	139952177142976 -> 139951966993328
	139951966993328 [label=AccumulateGrad]
	139951966993184 -> 139951966993280
	139952177142816 [label="Expert_Gate.expert_layers.0.bn1.bias
 (64)" fillcolor=lightblue]
	139952177142816 -> 139951966993184
	139951966993184 [label=AccumulateGrad]
	139951966993040 -> 139951966992944
	139951966993040 [label=ReluBackward0]
	139951966993520 -> 139951966993040
	139951966993520 [label=NativeGroupNormBackward0]
	139951966993616 -> 139951966993520
	139951966993616 [label=ConvolutionBackward0]
	139951966993808 -> 139951966993616
	139951949331968 [label="Expert_Gate.expert_layers.1.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	139951949331968 -> 139951966993808
	139951966993808 [label=AccumulateGrad]
	139951966993664 -> 139951966993520
	139951949332048 [label="Expert_Gate.expert_layers.1.bn1.weight
 (64)" fillcolor=lightblue]
	139951949332048 -> 139951966993664
	139951966993664 [label=AccumulateGrad]
	139951966993232 -> 139951966993520
	139951949332128 [label="Expert_Gate.expert_layers.1.bn1.bias
 (64)" fillcolor=lightblue]
	139951949332128 -> 139951966993232
	139951966993232 [label=AccumulateGrad]
	139951966992992 -> 139951966992944
	139951966992992 [label=ReluBackward0]
	139951966993760 -> 139951966992992
	139951966993760 [label=NativeGroupNormBackward0]
	139951966993856 -> 139951966993760
	139951966993856 [label=ConvolutionBackward0]
	139951966994048 -> 139951966993856
	139951823016560 [label="Expert_Gate.expert_layers.2.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	139951823016560 -> 139951966994048
	139951966994048 [label=AccumulateGrad]
	139951966993904 -> 139951966993760
	139951823016640 [label="Expert_Gate.expert_layers.2.bn1.weight
 (64)" fillcolor=lightblue]
	139951823016640 -> 139951966993904
	139951966993904 [label=AccumulateGrad]
	139951966993424 -> 139951966993760
	139951823016720 [label="Expert_Gate.expert_layers.2.bn1.bias
 (64)" fillcolor=lightblue]
	139951823016720 -> 139951966993424
	139951966993424 [label=AccumulateGrad]
	139951966992896 -> 139951966992752
	139951966992896 [label=AddBackward0]
	139951966993712 -> 139951966992896
	139951966993712 [label=TanhBackward0]
	139951966994144 -> 139951966993712
	139951966994144 [label=AddBackward0]
	139951966993952 -> 139951966994144
	139951966993952 [label=MulBackward0]
	139951966994288 -> 139951966993952
	139951966994288 [label=MulBackward0]
	139951966994384 -> 139951966994288
	139951966994384 [label=PowBackward0]
	139951967002832 -> 139951966994384
	139951967002832 [label=AddBackward0]
	139951967002928 -> 139951967002832
	139951967002928 [label=SumBackward1]
	139951967003024 -> 139951967002928
	139951967003024 [label=PowBackward0]
	139951966992944 -> 139951967003024
	139951966994336 -> 139951966994288
	139951798052320 [label="Expert_Gate.fusion_modules.0.alpha
 (1, 192, 1, 1)" fillcolor=lightblue]
	139951798052320 -> 139951966994336
	139951966994336 [label=AccumulateGrad]
	139951966994240 -> 139951966993952
	139951966994240 [label=DivBackward0]
	139951967002880 -> 139951966994240
	139951798052400 [label="Expert_Gate.fusion_modules.0.gamma
 (1, 192, 1, 1)" fillcolor=lightblue]
	139951798052400 -> 139951967002880
	139951967002880 [label=AccumulateGrad]
	139951967002976 -> 139951966994240
	139951967002976 [label=PowBackward0]
	139951967002784 -> 139951967002976
	139951967002784 [label=AddBackward0]
	139951967003168 -> 139951967002784
	139951967003168 [label=MeanBackward1]
	139951967003264 -> 139951967003168
	139951967003264 [label=PowBackward0]
	139951966994288 -> 139951967003264
	139951966994096 -> 139951966994144
	139951798052480 [label="Expert_Gate.fusion_modules.0.beta
 (1, 192, 1, 1)" fillcolor=lightblue]
	139951798052480 -> 139951966994096
	139951966994096 [label=AccumulateGrad]
	139951966992704 -> 139951966992560
	139951798054000 [label="Expert_Gate.fusion_modules.0.conv.weight
 (64, 192, 1, 1)" fillcolor=lightblue]
	139951798054000 -> 139951966992704
	139951966992704 [label=AccumulateGrad]
	139951966992656 -> 139951966992560
	139951798054080 [label="Expert_Gate.fusion_modules.0.conv.bias
 (64)" fillcolor=lightblue]
	139951798054080 -> 139951966992656
	139951966992656 [label=AccumulateGrad]
	139951966992512 -> 139951966992464
	139951798054160 [label="Expert_Gate.fusion_modules.0.bn.weight
 (64)" fillcolor=lightblue]
	139951798054160 -> 139951966992512
	139951966992512 [label=AccumulateGrad]
	139951966992368 -> 139951966992464
	139951798054240 [label="Expert_Gate.fusion_modules.0.bn.bias
 (64)" fillcolor=lightblue]
	139951798054240 -> 139951966992368
	139951966992368 [label=AccumulateGrad]
	139951966992272 -> 139951966992176
	139951966992272 [label=UpsampleBilinear2DBackward0]
	139951966992800 -> 139951966992272
	139951966992800 [label=AddBackward0]
	139951966993472 -> 139951966992800
	139951966993472 [label=ReluBackward0]
	139951966994192 -> 139951966993472
	139951966994192 [label=ConvolutionBackward0]
	139951966994000 -> 139951966994192
	139951966994000 [label=ReluBackward0]
	139951967003360 -> 139951966994000
	139951967003360 [label=ConvolutionBackward0]
	139951967003072 -> 139951967003360
	139951967003072 [label=CatBackward0]
	139951967003504 -> 139951967003072
	139951967003504 [label=ReluBackward0]
	139951967003648 -> 139951967003504
	139951967003648 [label=NativeGroupNormBackward0]
	139951967003744 -> 139951967003648
	139951967003744 [label=ConvolutionBackward0]
	139951967003936 -> 139951967003744
	139951967003936 [label=MulBackward0]
	139951967004128 -> 139951967003936
	139951967004128 [label=CatBackward0]
	139951967004272 -> 139951967004128
	139951967004272 [label=ReluBackward0]
	139951967004464 -> 139951967004272
	139951967004464 [label=AddBackward0]
	139951967004560 -> 139951967004464
	139951967004560 [label=NativeGroupNormBackward0]
	139951967004704 -> 139951967004560
	139951967004704 [label=ConvolutionBackward0]
	139951967004896 -> 139951967004704
	139951967004896 [label=ReluBackward0]
	139951967005040 -> 139951967004896
	139951967005040 [label=NativeGroupNormBackward0]
	139951967005136 -> 139951967005040
	139951967005136 [label=ConvolutionBackward0]
	139951967005328 -> 139951967005136
	139951967005328 [label=ReluBackward0]
	139951967005472 -> 139951967005328
	139951967005472 [label=NativeGroupNormBackward0]
	139951967005568 -> 139951967005472
	139951967005568 [label=ConvolutionBackward0]
	139951967004512 -> 139951967005568
	139951967004512 [label=ReluBackward0]
	139951967005856 -> 139951967004512
	139951967005856 [label=AddBackward0]
	139951967005952 -> 139951967005856
	139951967005952 [label=NativeGroupNormBackward0]
	139951967006096 -> 139951967005952
	139951967006096 [label=ConvolutionBackward0]
	139951967006288 -> 139951967006096
	139951967006288 [label=ReluBackward0]
	139951967006432 -> 139951967006288
	139951967006432 [label=NativeGroupNormBackward0]
	139951967006528 -> 139951967006432
	139951967006528 [label=ConvolutionBackward0]
	139951967006672 -> 139951967006528
	139951967006672 [label=ReluBackward0]
	139951967019216 -> 139951967006672
	139951967019216 [label=NativeGroupNormBackward0]
	139951967019312 -> 139951967019216
	139951967019312 [label=ConvolutionBackward0]
	139951967005904 -> 139951967019312
	139951967005904 [label=ReluBackward0]
	139951967019600 -> 139951967005904
	139951967019600 [label=AddBackward0]
	139951967019696 -> 139951967019600
	139951967019696 [label=NativeGroupNormBackward0]
	139951967019840 -> 139951967019696
	139951967019840 [label=ConvolutionBackward0]
	139951967020032 -> 139951967019840
	139951967020032 [label=ReluBackward0]
	139951967020176 -> 139951967020032
	139951967020176 [label=NativeGroupNormBackward0]
	139951967020272 -> 139951967020176
	139951967020272 [label=ConvolutionBackward0]
	139951967020464 -> 139951967020272
	139951967020464 [label=ReluBackward0]
	139951967020608 -> 139951967020464
	139951967020608 [label=NativeGroupNormBackward0]
	139951967020704 -> 139951967020608
	139951967020704 [label=ConvolutionBackward0]
	139951967020896 -> 139951967020704
	139951967020896 [label=MaxPool2DWithIndicesBackward0]
	139951966993088 -> 139951967020896
	139951967020848 -> 139951967020704
	139952177141536 [label="Expert_Gate.expert_layers.0.layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139952177141536 -> 139951967020848
	139951967020848 [label=AccumulateGrad]
	139951967020656 -> 139951967020608
	139952177147552 [label="Expert_Gate.expert_layers.0.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	139952177147552 -> 139951967020656
	139951967020656 [label=AccumulateGrad]
	139951967020512 -> 139951967020608
	139952177147392 [label="Expert_Gate.expert_layers.0.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	139952177147392 -> 139951967020512
	139951967020512 [label=AccumulateGrad]
	139951967020416 -> 139951967020272
	139952177146912 [label="Expert_Gate.expert_layers.0.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139952177146912 -> 139951967020416
	139951967020416 [label=AccumulateGrad]
	139951967020224 -> 139951967020176
	139952177147072 [label="Expert_Gate.expert_layers.0.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	139952177147072 -> 139951967020224
	139951967020224 [label=AccumulateGrad]
	139951967020080 -> 139951967020176
	139952177146752 [label="Expert_Gate.expert_layers.0.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	139952177146752 -> 139951967020080
	139951967020080 [label=AccumulateGrad]
	139951967019984 -> 139951967019840
	139952177146272 [label="Expert_Gate.expert_layers.0.layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139952177146272 -> 139951967019984
	139951967019984 [label=AccumulateGrad]
	139951967019792 -> 139951967019696
	139952177146432 [label="Expert_Gate.expert_layers.0.layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	139952177146432 -> 139951967019792
	139951967019792 [label=AccumulateGrad]
	139951967019744 -> 139951967019696
	139952177146112 [label="Expert_Gate.expert_layers.0.layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	139952177146112 -> 139951967019744
	139951967019744 [label=AccumulateGrad]
	139951967019648 -> 139951967019600
	139951967019648 [label=NativeGroupNormBackward0]
	139951967020368 -> 139951967019648
	139951967020368 [label=ConvolutionBackward0]
	139951967020896 -> 139951967020368
	139951967020752 -> 139951967020368
	139952177142496 [label="Expert_Gate.expert_layers.0.layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139952177142496 -> 139951967020752
	139951967020752 [label=AccumulateGrad]
	139951967019936 -> 139951967019648
	139952177142336 [label="Expert_Gate.expert_layers.0.layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	139952177142336 -> 139951967019936
	139951967019936 [label=AccumulateGrad]
	139951967019888 -> 139951967019648
	139952177142256 [label="Expert_Gate.expert_layers.0.layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	139952177142256 -> 139951967019888
	139951967019888 [label=AccumulateGrad]
	139951967019504 -> 139951967019312
	139952177145792 [label="Expert_Gate.expert_layers.0.layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	139952177145792 -> 139951967019504
	139951967019504 [label=AccumulateGrad]
	139951967019264 -> 139951967019216
	139952177145632 [label="Expert_Gate.expert_layers.0.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	139952177145632 -> 139951967019264
	139951967019264 [label=AccumulateGrad]
	139951967019120 -> 139951967019216
	139952177145472 [label="Expert_Gate.expert_layers.0.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	139952177145472 -> 139951967019120
	139951967019120 [label=AccumulateGrad]
	139951967006624 -> 139951967006528
	139952177144992 [label="Expert_Gate.expert_layers.0.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139952177144992 -> 139951967006624
	139951967006624 [label=AccumulateGrad]
	139951967006480 -> 139951967006432
	139952177145152 [label="Expert_Gate.expert_layers.0.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	139952177145152 -> 139951967006480
	139951967006480 [label=AccumulateGrad]
	139951967006336 -> 139951967006432
	139952177144832 [label="Expert_Gate.expert_layers.0.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	139952177144832 -> 139951967006336
	139951967006336 [label=AccumulateGrad]
	139951967006240 -> 139951967006096
	139952177144512 [label="Expert_Gate.expert_layers.0.layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139952177144512 -> 139951967006240
	139951967006240 [label=AccumulateGrad]
	139951967006048 -> 139951967005952
	139952177144352 [label="Expert_Gate.expert_layers.0.layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	139952177144352 -> 139951967006048
	139951967006048 [label=AccumulateGrad]
	139951967006000 -> 139951967005952
	139952177144032 [label="Expert_Gate.expert_layers.0.layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	139952177144032 -> 139951967006000
	139951967006000 [label=AccumulateGrad]
	139951967005904 -> 139951967005856
	139951967005760 -> 139951967005568
	139952177143872 [label="Expert_Gate.expert_layers.0.layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	139952177143872 -> 139951967005760
	139951967005760 [label=AccumulateGrad]
	139951967005520 -> 139951967005472
	139952177147712 [label="Expert_Gate.expert_layers.0.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	139952177147712 -> 139951967005520
	139951967005520 [label=AccumulateGrad]
	139951967005376 -> 139951967005472
	139952177108528 [label="Expert_Gate.expert_layers.0.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	139952177108528 -> 139951967005376
	139951967005376 [label=AccumulateGrad]
	139951967005280 -> 139951967005136
	139952177108368 [label="Expert_Gate.expert_layers.0.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139952177108368 -> 139951967005280
	139951967005280 [label=AccumulateGrad]
	139951967005088 -> 139951967005040
	139952177108288 [label="Expert_Gate.expert_layers.0.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	139952177108288 -> 139951967005088
	139951967005088 [label=AccumulateGrad]
	139951967004944 -> 139951967005040
	139952177108048 [label="Expert_Gate.expert_layers.0.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	139952177108048 -> 139951967004944
	139951967004944 [label=AccumulateGrad]
	139951967004848 -> 139951967004704
	139952177107808 [label="Expert_Gate.expert_layers.0.layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139952177107808 -> 139951967004848
	139951967004848 [label=AccumulateGrad]
	139951967004656 -> 139951967004560
	139952177107888 [label="Expert_Gate.expert_layers.0.layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	139952177107888 -> 139951967004656
	139951967004656 [label=AccumulateGrad]
	139951967004608 -> 139951967004560
	139952177107568 [label="Expert_Gate.expert_layers.0.layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	139952177107568 -> 139951967004608
	139951967004608 [label=AccumulateGrad]
	139951967004512 -> 139951967004464
	139951967004224 -> 139951967004128
	139951967004224 [label=ReluBackward0]
	139951967004752 -> 139951967004224
	139951967004752 [label=AddBackward0]
	139951967005232 -> 139951967004752
	139951967005232 [label=NativeGroupNormBackward0]
	139951967005712 -> 139951967005232
	139951967005712 [label=ConvolutionBackward0]
	139951967005664 -> 139951967005712
	139951967005664 [label=ReluBackward0]
	139951967006576 -> 139951967005664
	139951967006576 [label=NativeGroupNormBackward0]
	139951967006384 -> 139951967006576
	139951967006384 [label=ConvolutionBackward0]
	139951967020320 -> 139951967006384
	139951967020320 [label=ReluBackward0]
	139951967020800 -> 139951967020320
	139951967020800 [label=NativeGroupNormBackward0]
	139951967020560 -> 139951967020800
	139951967020560 [label=ConvolutionBackward0]
	139951967004800 -> 139951967020560
	139951967004800 [label=ReluBackward0]
	139951967021184 -> 139951967004800
	139951967021184 [label=AddBackward0]
	139951967021280 -> 139951967021184
	139951967021280 [label=NativeGroupNormBackward0]
	139951967021424 -> 139951967021280
	139951967021424 [label=ConvolutionBackward0]
	139951967021616 -> 139951967021424
	139951967021616 [label=ReluBackward0]
	139951967021760 -> 139951967021616
	139951967021760 [label=NativeGroupNormBackward0]
	139951967021856 -> 139951967021760
	139951967021856 [label=ConvolutionBackward0]
	139951967022048 -> 139951967021856
	139951967022048 [label=ReluBackward0]
	139951967022192 -> 139951967022048
	139951967022192 [label=NativeGroupNormBackward0]
	139951967022288 -> 139951967022192
	139951967022288 [label=ConvolutionBackward0]
	139951967021232 -> 139951967022288
	139951967021232 [label=ReluBackward0]
	139951967022576 -> 139951967021232
	139951967022576 [label=AddBackward0]
	139951967022672 -> 139951967022576
	139951967022672 [label=NativeGroupNormBackward0]
	139951967022816 -> 139951967022672
	139951967022816 [label=ConvolutionBackward0]
	139951967023008 -> 139951967022816
	139951967023008 [label=ReluBackward0]
	139951967023056 -> 139951967023008
	139951967023056 [label=NativeGroupNormBackward0]
	139951966523600 -> 139951967023056
	139951966523600 [label=ConvolutionBackward0]
	139951966523792 -> 139951966523600
	139951966523792 [label=ReluBackward0]
	139951966523936 -> 139951966523792
	139951966523936 [label=NativeGroupNormBackward0]
	139951966524032 -> 139951966523936
	139951966524032 [label=ConvolutionBackward0]
	139951966524224 -> 139951966524032
	139951966524224 [label=MaxPool2DWithIndicesBackward0]
	139951966993040 -> 139951966524224
	139951966524176 -> 139951966524032
	139951895748832 [label="Expert_Gate.expert_layers.1.layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139951895748832 -> 139951966524176
	139951966524176 [label=AccumulateGrad]
	139951966523984 -> 139951966523936
	139951895748912 [label="Expert_Gate.expert_layers.1.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	139951895748912 -> 139951966523984
	139951966523984 [label=AccumulateGrad]
	139951966523840 -> 139951966523936
	139951895748992 [label="Expert_Gate.expert_layers.1.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	139951895748992 -> 139951966523840
	139951966523840 [label=AccumulateGrad]
	139951966523744 -> 139951966523600
	139951895749232 [label="Expert_Gate.expert_layers.1.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139951895749232 -> 139951966523744
	139951966523744 [label=AccumulateGrad]
	139951966523552 -> 139951967023056
	139951895749152 [label="Expert_Gate.expert_layers.1.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	139951895749152 -> 139951966523552
	139951966523552 [label=AccumulateGrad]
	139951966523504 -> 139951967023056
	139951895749312 [label="Expert_Gate.expert_layers.1.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	139951895749312 -> 139951966523504
	139951966523504 [label=AccumulateGrad]
	139951967022960 -> 139951967022816
	139951895749472 [label="Expert_Gate.expert_layers.1.layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139951895749472 -> 139951967022960
	139951967022960 [label=AccumulateGrad]
	139951967022768 -> 139951967022672
	139951895749552 [label="Expert_Gate.expert_layers.1.layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	139951895749552 -> 139951967022768
	139951967022768 [label=AccumulateGrad]
	139951967022720 -> 139951967022672
	139951895749632 [label="Expert_Gate.expert_layers.1.layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	139951895749632 -> 139951967022720
	139951967022720 [label=AccumulateGrad]
	139951967022624 -> 139951967022576
	139951967022624 [label=NativeGroupNormBackward0]
	139951967022912 -> 139951967022624
	139951967022912 [label=ConvolutionBackward0]
	139951966524224 -> 139951967022912
	139951966523888 -> 139951967022912
	139951949332288 [label="Expert_Gate.expert_layers.1.layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139951949332288 -> 139951966523888
	139951966523888 [label=AccumulateGrad]
	139951967022864 -> 139951967022624
	139951949332368 [label="Expert_Gate.expert_layers.1.layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	139951949332368 -> 139951967022864
	139951967022864 [label=AccumulateGrad]
	139951966523648 -> 139951967022624
	139951895748672 [label="Expert_Gate.expert_layers.1.layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	139951895748672 -> 139951966523648
	139951966523648 [label=AccumulateGrad]
	139951967022480 -> 139951967022288
	139951895749792 [label="Expert_Gate.expert_layers.1.layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	139951895749792 -> 139951967022480
	139951967022480 [label=AccumulateGrad]
	139951967022240 -> 139951967022192
	139951895749872 [label="Expert_Gate.expert_layers.1.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	139951895749872 -> 139951967022240
	139951967022240 [label=AccumulateGrad]
	139951967022096 -> 139951967022192
	139951895749952 [label="Expert_Gate.expert_layers.1.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	139951895749952 -> 139951967022096
	139951967022096 [label=AccumulateGrad]
	139951967022000 -> 139951967021856
	139951895750192 [label="Expert_Gate.expert_layers.1.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139951895750192 -> 139951967022000
	139951967022000 [label=AccumulateGrad]
	139951967021808 -> 139951967021760
	139951895750112 [label="Expert_Gate.expert_layers.1.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	139951895750112 -> 139951967021808
	139951967021808 [label=AccumulateGrad]
	139951967021664 -> 139951967021760
	139951895750272 [label="Expert_Gate.expert_layers.1.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	139951895750272 -> 139951967021664
	139951967021664 [label=AccumulateGrad]
	139951967021568 -> 139951967021424
	139951895750432 [label="Expert_Gate.expert_layers.1.layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139951895750432 -> 139951967021568
	139951967021568 [label=AccumulateGrad]
	139951967021376 -> 139951967021280
	139951895750512 [label="Expert_Gate.expert_layers.1.layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	139951895750512 -> 139951967021376
	139951967021376 [label=AccumulateGrad]
	139951967021328 -> 139951967021280
	139951895750592 [label="Expert_Gate.expert_layers.1.layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	139951895750592 -> 139951967021328
	139951967021328 [label=AccumulateGrad]
	139951967021232 -> 139951967021184
	139951967021088 -> 139951967020560
	139951895750752 [label="Expert_Gate.expert_layers.1.layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	139951895750752 -> 139951967021088
	139951967021088 [label=AccumulateGrad]
	139951967020944 -> 139951967020800
	139951895750832 [label="Expert_Gate.expert_layers.1.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	139951895750832 -> 139951967020944
	139951967020944 [label=AccumulateGrad]
	139951967019408 -> 139951967020800
	139951895750912 [label="Expert_Gate.expert_layers.1.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	139951895750912 -> 139951967019408
	139951967019408 [label=AccumulateGrad]
	139951967019168 -> 139951967006384
	139951895751152 [label="Expert_Gate.expert_layers.1.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139951895751152 -> 139951967019168
	139951967019168 [label=AccumulateGrad]
	139951967005808 -> 139951967006576
	139951895751072 [label="Expert_Gate.expert_layers.1.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	139951895751072 -> 139951967005808
	139951967005808 [label=AccumulateGrad]
	139951967019072 -> 139951967006576
	139951895751232 [label="Expert_Gate.expert_layers.1.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	139951895751232 -> 139951967019072
	139951967019072 [label=AccumulateGrad]
	139951967006144 -> 139951967005712
	139951895751392 [label="Expert_Gate.expert_layers.1.layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139951895751392 -> 139951967006144
	139951967006144 [label=AccumulateGrad]
	139951967004992 -> 139951967005232
	139951895751472 [label="Expert_Gate.expert_layers.1.layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	139951895751472 -> 139951967004992
	139951967004992 [label=AccumulateGrad]
	139951967005184 -> 139951967005232
	139951895751552 [label="Expert_Gate.expert_layers.1.layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	139951895751552 -> 139951967005184
	139951967005184 [label=AccumulateGrad]
	139951967004800 -> 139951967004752
	139951967004176 -> 139951967004128
	139951967004176 [label=ReluBackward0]
	139951967005616 -> 139951967004176
	139951967005616 [label=AddBackward0]
	139951967006192 -> 139951967005616
	139951967006192 [label=NativeGroupNormBackward0]
	139951967021040 -> 139951967006192
	139951967021040 [label=ConvolutionBackward0]
	139951967020992 -> 139951967021040
	139951967020992 [label=ReluBackward0]
	139951967021952 -> 139951967020992
	139951967021952 [label=NativeGroupNormBackward0]
	139951967021712 -> 139951967021952
	139951967021712 [label=ConvolutionBackward0]
	139951967022384 -> 139951967021712
	139951967022384 [label=ReluBackward0]
	139951967022528 -> 139951967022384
	139951967022528 [label=NativeGroupNormBackward0]
	139951966524272 -> 139951967022528
	139951966524272 [label=ConvolutionBackward0]
	139951967005424 -> 139951966524272
	139951967005424 [label=ReluBackward0]
	139951966524560 -> 139951967005424
	139951966524560 [label=AddBackward0]
	139951966524656 -> 139951966524560
	139951966524656 [label=NativeGroupNormBackward0]
	139951966524800 -> 139951966524656
	139951966524800 [label=ConvolutionBackward0]
	139951966524992 -> 139951966524800
	139951966524992 [label=ReluBackward0]
	139951966525136 -> 139951966524992
	139951966525136 [label=NativeGroupNormBackward0]
	139951966525232 -> 139951966525136
	139951966525232 [label=ConvolutionBackward0]
	139951966525424 -> 139951966525232
	139951966525424 [label=ReluBackward0]
	139951966525568 -> 139951966525424
	139951966525568 [label=NativeGroupNormBackward0]
	139951966525664 -> 139951966525568
	139951966525664 [label=ConvolutionBackward0]
	139951966524608 -> 139951966525664
	139951966524608 [label=ReluBackward0]
	139951966525952 -> 139951966524608
	139951966525952 [label=AddBackward0]
	139951966526048 -> 139951966525952
	139951966526048 [label=NativeGroupNormBackward0]
	139951966526192 -> 139951966526048
	139951966526192 [label=ConvolutionBackward0]
	139951966526384 -> 139951966526192
	139951966526384 [label=ReluBackward0]
	139951966526528 -> 139951966526384
	139951966526528 [label=NativeGroupNormBackward0]
	139951966526624 -> 139951966526528
	139951966526624 [label=ConvolutionBackward0]
	139951966526816 -> 139951966526624
	139951966526816 [label=ReluBackward0]
	139951966526960 -> 139951966526816
	139951966526960 [label=NativeGroupNormBackward0]
	139951966527056 -> 139951966526960
	139951966527056 [label=ConvolutionBackward0]
	139951966527248 -> 139951966527056
	139951966527248 [label=MaxPool2DWithIndicesBackward0]
	139951966992992 -> 139951966527248
	139951966527200 -> 139951966527056
	139951823017200 [label="Expert_Gate.expert_layers.2.layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	139951823017200 -> 139951966527200
	139951966527200 [label=AccumulateGrad]
	139951966527008 -> 139951966526960
	139951823017280 [label="Expert_Gate.expert_layers.2.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	139951823017280 -> 139951966527008
	139951966527008 [label=AccumulateGrad]
	139951966526864 -> 139951966526960
	139951823017360 [label="Expert_Gate.expert_layers.2.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	139951823017360 -> 139951966526864
	139951966526864 [label=AccumulateGrad]
	139951966526768 -> 139951966526624
	139951823017600 [label="Expert_Gate.expert_layers.2.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139951823017600 -> 139951966526768
	139951966526768 [label=AccumulateGrad]
	139951966526576 -> 139951966526528
	139951823017520 [label="Expert_Gate.expert_layers.2.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	139951823017520 -> 139951966526576
	139951966526576 [label=AccumulateGrad]
	139951966526432 -> 139951966526528
	139951823017680 [label="Expert_Gate.expert_layers.2.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	139951823017680 -> 139951966526432
	139951966526432 [label=AccumulateGrad]
	139951966526336 -> 139951966526192
	139951823017840 [label="Expert_Gate.expert_layers.2.layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139951823017840 -> 139951966526336
	139951966526336 [label=AccumulateGrad]
	139951966526144 -> 139951966526048
	139951823017920 [label="Expert_Gate.expert_layers.2.layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	139951823017920 -> 139951966526144
	139951966526144 [label=AccumulateGrad]
	139951966526096 -> 139951966526048
	139951823018000 [label="Expert_Gate.expert_layers.2.layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	139951823018000 -> 139951966526096
	139951966526096 [label=AccumulateGrad]
	139951966526000 -> 139951966525952
	139951966526000 [label=NativeGroupNormBackward0]
	139951966526720 -> 139951966526000
	139951966526720 [label=ConvolutionBackward0]
	139951966527248 -> 139951966526720
	139951966527104 -> 139951966526720
	139951823016880 [label="Expert_Gate.expert_layers.2.layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139951823016880 -> 139951966527104
	139951966527104 [label=AccumulateGrad]
	139951966526288 -> 139951966526000
	139951823016960 [label="Expert_Gate.expert_layers.2.layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	139951823016960 -> 139951966526288
	139951966526288 [label=AccumulateGrad]
	139951966526240 -> 139951966526000
	139951823017040 [label="Expert_Gate.expert_layers.2.layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	139951823017040 -> 139951966526240
	139951966526240 [label=AccumulateGrad]
	139951966525856 -> 139951966525664
	139951823018160 [label="Expert_Gate.expert_layers.2.layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	139951823018160 -> 139951966525856
	139951966525856 [label=AccumulateGrad]
	139951966525616 -> 139951966525568
	139951823018240 [label="Expert_Gate.expert_layers.2.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	139951823018240 -> 139951966525616
	139951966525616 [label=AccumulateGrad]
	139951966525472 -> 139951966525568
	139951823018320 [label="Expert_Gate.expert_layers.2.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	139951823018320 -> 139951966525472
	139951966525472 [label=AccumulateGrad]
	139951966525376 -> 139951966525232
	139951823018560 [label="Expert_Gate.expert_layers.2.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139951823018560 -> 139951966525376
	139951966525376 [label=AccumulateGrad]
	139951966525184 -> 139951966525136
	139951823018480 [label="Expert_Gate.expert_layers.2.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	139951823018480 -> 139951966525184
	139951966525184 [label=AccumulateGrad]
	139951966525040 -> 139951966525136
	139951823018640 [label="Expert_Gate.expert_layers.2.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	139951823018640 -> 139951966525040
	139951966525040 [label=AccumulateGrad]
	139951966524944 -> 139951966524800
	139951823018800 [label="Expert_Gate.expert_layers.2.layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139951823018800 -> 139951966524944
	139951966524944 [label=AccumulateGrad]
	139951966524752 -> 139951966524656
	139951823018880 [label="Expert_Gate.expert_layers.2.layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	139951823018880 -> 139951966524752
	139951966524752 [label=AccumulateGrad]
	139951966524704 -> 139951966524656
	139951823018960 [label="Expert_Gate.expert_layers.2.layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	139951823018960 -> 139951966524704
	139951966524704 [label=AccumulateGrad]
	139951966524608 -> 139951966524560
	139951966524464 -> 139951966524272
	139951823019120 [label="Expert_Gate.expert_layers.2.layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	139951823019120 -> 139951966524464
	139951966524464 [label=AccumulateGrad]
	139951966524320 -> 139951967022528
	139951823019200 [label="Expert_Gate.expert_layers.2.layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	139951823019200 -> 139951966524320
	139951966524320 [label=AccumulateGrad]
	139951966523456 -> 139951967022528
	139951823019280 [label="Expert_Gate.expert_layers.2.layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	139951823019280 -> 139951966523456
	139951966523456 [label=AccumulateGrad]
	139951967022144 -> 139951967021712
	139951823019520 [label="Expert_Gate.expert_layers.2.layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139951823019520 -> 139951967022144
	139951967022144 [label=AccumulateGrad]
	139951967021904 -> 139951967021952
	139951823019440 [label="Expert_Gate.expert_layers.2.layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	139951823019440 -> 139951967021904
	139951967021904 [label=AccumulateGrad]
	139951967021136 -> 139951967021952
	139951823019600 [label="Expert_Gate.expert_layers.2.layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	139951823019600 -> 139951967021136
	139951967021136 [label=AccumulateGrad]
	139951967021472 -> 139951967021040
	139951823019760 [label="Expert_Gate.expert_layers.2.layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	139951823019760 -> 139951967021472
	139951967021472 [label=AccumulateGrad]
	139951967019456 -> 139951967006192
	139951823019840 [label="Expert_Gate.expert_layers.2.layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	139951823019840 -> 139951967019456
	139951967019456 [label=AccumulateGrad]
	139951967019360 -> 139951967006192
	139951823019920 [label="Expert_Gate.expert_layers.2.layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	139951823019920 -> 139951967019360
	139951967019360 [label=AccumulateGrad]
	139951967005424 -> 139951967005616
	139951967004080 -> 139951967003936
	139951967004080 [label=AddBackward0]
	139951967004368 -> 139951967004080
	139951967004368 [label=TanhBackward0]
	139951967004320 -> 139951967004368
	139951967004320 [label=AddBackward0]
	139951967022432 -> 139951967004320
	139951967022432 [label=MulBackward0]
	139951967021520 -> 139951967022432
	139951967021520 [label=MulBackward0]
	139951966524368 -> 139951967021520
	139951966524368 [label=PowBackward0]
	139951966525328 -> 139951966524368
	139951966525328 [label=AddBackward0]
	139951966525088 -> 139951966525328
	139951966525088 [label=SumBackward1]
	139951966525712 -> 139951966525088
	139951966525712 [label=PowBackward0]
	139951967004128 -> 139951966525712
	139951966524848 -> 139951967021520
	139951798054320 [label="Expert_Gate.fusion_modules.1.alpha
 (1, 768, 1, 1)" fillcolor=lightblue]
	139951798054320 -> 139951966524848
	139951966524848 [label=AccumulateGrad]
	139951966524128 -> 139951967022432
	139951966524128 [label=DivBackward0]
	139951966525280 -> 139951966524128
	139951798054400 [label="Expert_Gate.fusion_modules.1.gamma
 (1, 768, 1, 1)" fillcolor=lightblue]
	139951798054400 -> 139951966525280
	139951966525280 [label=AccumulateGrad]
	139951966525808 -> 139951966524128
	139951966525808 [label=PowBackward0]
	139951966524896 -> 139951966525808
	139951966524896 [label=AddBackward0]
	139951966525760 -> 139951966524896
	139951966525760 [label=MeanBackward1]
	139951966527152 -> 139951966525760
	139951966527152 [label=PowBackward0]
	139951967021520 -> 139951966527152
	139951967022336 -> 139951967004320
	139951798054480 [label="Expert_Gate.fusion_modules.1.beta
 (1, 768, 1, 1)" fillcolor=lightblue]
	139951798054480 -> 139951967022336
	139951967022336 [label=AccumulateGrad]
	139951967003888 -> 139951967003744
	139951798054800 [label="Expert_Gate.fusion_modules.1.conv.weight
 (256, 768, 1, 1)" fillcolor=lightblue]
	139951798054800 -> 139951967003888
	139951967003888 [label=AccumulateGrad]
	139951967003840 -> 139951967003744
	139951746236480 [label="Expert_Gate.fusion_modules.1.conv.bias
 (256)" fillcolor=lightblue]
	139951746236480 -> 139951967003840
	139951967003840 [label=AccumulateGrad]
	139951967003696 -> 139951967003648
	139951798053120 [label="Expert_Gate.fusion_modules.1.bn.weight
 (256)" fillcolor=lightblue]
	139951798053120 -> 139951967003696
	139951967003696 [label=AccumulateGrad]
	139951967003552 -> 139951967003648
	139951798052000 [label="Expert_Gate.fusion_modules.1.bn.bias
 (256)" fillcolor=lightblue]
	139951798052000 -> 139951967003552
	139951967003552 [label=AccumulateGrad]
	139951967003456 -> 139951967003072
	139951967003456 [label=UpsampleBilinear2DBackward0]
	139951967003984 -> 139951967003456
	139951967003984 [label=AddBackward0]
	139951967004416 -> 139951967003984
	139951967004416 [label=ReluBackward0]
	139951967020128 -> 139951967004416
	139951967020128 [label=ConvolutionBackward0]
	139951966524416 -> 139951967020128
	139951966524416 [label=ReluBackward0]
	139951966526912 -> 139951966524416
	139951966526912 [label=ConvolutionBackward0]
	139951966525520 -> 139951966526912
	139951966525520 [label=CatBackward0]
	139951966527392 -> 139951966525520
	139951966527392 [label=ReluBackward0]
	139951966527440 -> 139951966527392
	139951966527440 [label=NativeGroupNormBackward0]
	139951966564560 -> 139951966527440
	139951966564560 [label=ConvolutionBackward0]
	139951966564752 -> 139951966564560
	139951966564752 [label=MulBackward0]
	139951966564944 -> 139951966564752
	139951966564944 [label=CatBackward0]
	139951966565088 -> 139951966564944
	139951966565088 [label=ReluBackward0]
	139951966565280 -> 139951966565088
	139951966565280 [label=AddBackward0]
	139951966565376 -> 139951966565280
	139951966565376 [label=NativeGroupNormBackward0]
	139951966565520 -> 139951966565376
	139951966565520 [label=ConvolutionBackward0]
	139951966565712 -> 139951966565520
	139951966565712 [label=ReluBackward0]
	139951966565856 -> 139951966565712
	139951966565856 [label=NativeGroupNormBackward0]
	139951966565952 -> 139951966565856
	139951966565952 [label=ConvolutionBackward0]
	139951966566144 -> 139951966565952
	139951966566144 [label=ReluBackward0]
	139951966566288 -> 139951966566144
	139951966566288 [label=NativeGroupNormBackward0]
	139951966566384 -> 139951966566288
	139951966566384 [label=ConvolutionBackward0]
	139951966565328 -> 139951966566384
	139951966565328 [label=ReluBackward0]
	139951966566672 -> 139951966565328
	139951966566672 [label=AddBackward0]
	139951966566768 -> 139951966566672
	139951966566768 [label=NativeGroupNormBackward0]
	139951966566912 -> 139951966566768
	139951966566912 [label=ConvolutionBackward0]
	139951966567104 -> 139951966566912
	139951966567104 [label=ReluBackward0]
	139951966567248 -> 139951966567104
	139951966567248 [label=NativeGroupNormBackward0]
	139951966567344 -> 139951966567248
	139951966567344 [label=ConvolutionBackward0]
	139951966567536 -> 139951966567344
	139951966567536 [label=ReluBackward0]
	139951966567680 -> 139951966567536
	139951966567680 [label=NativeGroupNormBackward0]
	139951966567776 -> 139951966567680
	139951966567776 [label=ConvolutionBackward0]
	139951966566720 -> 139951966567776
	139951966566720 [label=ReluBackward0]
	139951966568064 -> 139951966566720
	139951966568064 [label=AddBackward0]
	139951966568160 -> 139951966568064
	139951966568160 [label=NativeGroupNormBackward0]
	139951966568304 -> 139951966568160
	139951966568304 [label=ConvolutionBackward0]
	139951966568400 -> 139951966568304
	139951966568400 [label=ReluBackward0]
	139951966572800 -> 139951966568400
	139951966572800 [label=NativeGroupNormBackward0]
	139951966572896 -> 139951966572800
	139951966572896 [label=ConvolutionBackward0]
	139951966573088 -> 139951966572896
	139951966573088 [label=ReluBackward0]
	139951966573232 -> 139951966573088
	139951966573232 [label=NativeGroupNormBackward0]
	139951966573328 -> 139951966573232
	139951966573328 [label=ConvolutionBackward0]
	139951966568112 -> 139951966573328
	139951966568112 [label=ReluBackward0]
	139951966573616 -> 139951966568112
	139951966573616 [label=AddBackward0]
	139951966573712 -> 139951966573616
	139951966573712 [label=NativeGroupNormBackward0]
	139951966573856 -> 139951966573712
	139951966573856 [label=ConvolutionBackward0]
	139951966574048 -> 139951966573856
	139951966574048 [label=ReluBackward0]
	139951966574192 -> 139951966574048
	139951966574192 [label=NativeGroupNormBackward0]
	139951966574288 -> 139951966574192
	139951966574288 [label=ConvolutionBackward0]
	139951966574480 -> 139951966574288
	139951966574480 [label=ReluBackward0]
	139951966574624 -> 139951966574480
	139951966574624 [label=NativeGroupNormBackward0]
	139951966574720 -> 139951966574624
	139951966574720 [label=ConvolutionBackward0]
	139951967004272 -> 139951966574720
	139951966574912 -> 139951966574720
	139952177108688 [label="Expert_Gate.expert_layers.0.layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	139952177108688 -> 139951966574912
	139951966574912 [label=AccumulateGrad]
	139951966574672 -> 139951966574624
	139952177108848 [label="Expert_Gate.expert_layers.0.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	139952177108848 -> 139951966574672
	139951966574672 [label=AccumulateGrad]
	139951966574528 -> 139951966574624
	139952120171072 [label="Expert_Gate.expert_layers.0.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	139952120171072 -> 139951966574528
	139951966574528 [label=AccumulateGrad]
	139951966574432 -> 139951966574288
	139952120171312 [label="Expert_Gate.expert_layers.0.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139952120171312 -> 139951966574432
	139951966574432 [label=AccumulateGrad]
	139951966574240 -> 139951966574192
	139952120171232 [label="Expert_Gate.expert_layers.0.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	139952120171232 -> 139951966574240
	139951966574240 [label=AccumulateGrad]
	139951966574096 -> 139951966574192
	139952120171392 [label="Expert_Gate.expert_layers.0.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	139952120171392 -> 139951966574096
	139951966574096 [label=AccumulateGrad]
	139951966574000 -> 139951966573856
	139952120171552 [label="Expert_Gate.expert_layers.0.layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139952120171552 -> 139951966574000
	139951966574000 [label=AccumulateGrad]
	139951966573808 -> 139951966573712
	139952120171632 [label="Expert_Gate.expert_layers.0.layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	139952120171632 -> 139951966573808
	139951966573808 [label=AccumulateGrad]
	139951966573760 -> 139951966573712
	139952120171712 [label="Expert_Gate.expert_layers.0.layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	139952120171712 -> 139951966573760
	139951966573760 [label=AccumulateGrad]
	139951966573664 -> 139951966573616
	139951966573664 [label=NativeGroupNormBackward0]
	139951966574384 -> 139951966573664
	139951966574384 [label=ConvolutionBackward0]
	139951967004272 -> 139951966574384
	139951966574768 -> 139951966574384
	139952177107328 [label="Expert_Gate.expert_layers.0.layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	139952177107328 -> 139951966574768
	139951966574768 [label=AccumulateGrad]
	139951966573952 -> 139951966573664
	139952177107408 [label="Expert_Gate.expert_layers.0.layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	139952177107408 -> 139951966573952
	139951966573952 [label=AccumulateGrad]
	139951966573904 -> 139951966573664
	139952177107008 [label="Expert_Gate.expert_layers.0.layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	139952177107008 -> 139951966573904
	139951966573904 [label=AccumulateGrad]
	139951966573520 -> 139951966573328
	139952120171872 [label="Expert_Gate.expert_layers.0.layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139952120171872 -> 139951966573520
	139951966573520 [label=AccumulateGrad]
	139951966573280 -> 139951966573232
	139952120171952 [label="Expert_Gate.expert_layers.0.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	139952120171952 -> 139951966573280
	139951966573280 [label=AccumulateGrad]
	139951966573136 -> 139951966573232
	139952120172032 [label="Expert_Gate.expert_layers.0.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	139952120172032 -> 139951966573136
	139951966573136 [label=AccumulateGrad]
	139951966573040 -> 139951966572896
	139952120172272 [label="Expert_Gate.expert_layers.0.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139952120172272 -> 139951966573040
	139951966573040 [label=AccumulateGrad]
	139951966572848 -> 139951966572800
	139952120172192 [label="Expert_Gate.expert_layers.0.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	139952120172192 -> 139951966572848
	139951966572848 [label=AccumulateGrad]
	139951966572704 -> 139951966572800
	139952120172352 [label="Expert_Gate.expert_layers.0.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	139952120172352 -> 139951966572704
	139951966572704 [label=AccumulateGrad]
	139951966572656 -> 139951966568304
	139951958691904 [label="Expert_Gate.expert_layers.0.layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951958691904 -> 139951966572656
	139951966572656 [label=AccumulateGrad]
	139951966568256 -> 139951966568160
	139951958691984 [label="Expert_Gate.expert_layers.0.layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	139951958691984 -> 139951966568256
	139951966568256 [label=AccumulateGrad]
	139951966568208 -> 139951966568160
	139951958692064 [label="Expert_Gate.expert_layers.0.layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	139951958692064 -> 139951966568208
	139951966568208 [label=AccumulateGrad]
	139951966568112 -> 139951966568064
	139951966567968 -> 139951966567776
	139951958692224 [label="Expert_Gate.expert_layers.0.layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139951958692224 -> 139951966567968
	139951966567968 [label=AccumulateGrad]
	139951966567728 -> 139951966567680
	139951958692304 [label="Expert_Gate.expert_layers.0.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	139951958692304 -> 139951966567728
	139951966567728 [label=AccumulateGrad]
	139951966567584 -> 139951966567680
	139951958692384 [label="Expert_Gate.expert_layers.0.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	139951958692384 -> 139951966567584
	139951966567584 [label=AccumulateGrad]
	139951966567488 -> 139951966567344
	139951958692624 [label="Expert_Gate.expert_layers.0.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139951958692624 -> 139951966567488
	139951966567488 [label=AccumulateGrad]
	139951966567296 -> 139951966567248
	139951958692544 [label="Expert_Gate.expert_layers.0.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	139951958692544 -> 139951966567296
	139951966567296 [label=AccumulateGrad]
	139951966567152 -> 139951966567248
	139951958692704 [label="Expert_Gate.expert_layers.0.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	139951958692704 -> 139951966567152
	139951966567152 [label=AccumulateGrad]
	139951966567056 -> 139951966566912
	139951958692864 [label="Expert_Gate.expert_layers.0.layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951958692864 -> 139951966567056
	139951966567056 [label=AccumulateGrad]
	139951966566864 -> 139951966566768
	139951958692944 [label="Expert_Gate.expert_layers.0.layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	139951958692944 -> 139951966566864
	139951966566864 [label=AccumulateGrad]
	139951966566816 -> 139951966566768
	139951958693024 [label="Expert_Gate.expert_layers.0.layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	139951958693024 -> 139951966566816
	139951966566816 [label=AccumulateGrad]
	139951966566720 -> 139951966566672
	139951966566576 -> 139951966566384
	139951958693184 [label="Expert_Gate.expert_layers.0.layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139951958693184 -> 139951966566576
	139951966566576 [label=AccumulateGrad]
	139951966566336 -> 139951966566288
	139951958693264 [label="Expert_Gate.expert_layers.0.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	139951958693264 -> 139951966566336
	139951966566336 [label=AccumulateGrad]
	139951966566192 -> 139951966566288
	139951958693344 [label="Expert_Gate.expert_layers.0.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	139951958693344 -> 139951966566192
	139951966566192 [label=AccumulateGrad]
	139951966566096 -> 139951966565952
	139951958693584 [label="Expert_Gate.expert_layers.0.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139951958693584 -> 139951966566096
	139951966566096 [label=AccumulateGrad]
	139951966565904 -> 139951966565856
	139951958693504 [label="Expert_Gate.expert_layers.0.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	139951958693504 -> 139951966565904
	139951966565904 [label=AccumulateGrad]
	139951966565760 -> 139951966565856
	139951958693664 [label="Expert_Gate.expert_layers.0.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	139951958693664 -> 139951966565760
	139951966565760 [label=AccumulateGrad]
	139951966565664 -> 139951966565520
	139951958693824 [label="Expert_Gate.expert_layers.0.layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951958693824 -> 139951966565664
	139951966565664 [label=AccumulateGrad]
	139951966565472 -> 139951966565376
	139951958693904 [label="Expert_Gate.expert_layers.0.layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	139951958693904 -> 139951966565472
	139951966565472 [label=AccumulateGrad]
	139951966565424 -> 139951966565376
	139951958693984 [label="Expert_Gate.expert_layers.0.layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	139951958693984 -> 139951966565424
	139951966565424 [label=AccumulateGrad]
	139951966565328 -> 139951966565280
	139951966565040 -> 139951966564944
	139951966565040 [label=ReluBackward0]
	139951966565568 -> 139951966565040
	139951966565568 [label=AddBackward0]
	139951966566048 -> 139951966565568
	139951966566048 [label=NativeGroupNormBackward0]
	139951966566528 -> 139951966566048
	139951966566528 [label=ConvolutionBackward0]
	139951966566480 -> 139951966566528
	139951966566480 [label=ReluBackward0]
	139951966567440 -> 139951966566480
	139951966567440 [label=NativeGroupNormBackward0]
	139951966567200 -> 139951966567440
	139951966567200 [label=ConvolutionBackward0]
	139951966567920 -> 139951966567200
	139951966567920 [label=ReluBackward0]
	139951966568352 -> 139951966567920
	139951966568352 [label=NativeGroupNormBackward0]
	139951966572752 -> 139951966568352
	139951966572752 [label=ConvolutionBackward0]
	139951966565616 -> 139951966572752
	139951966565616 [label=ReluBackward0]
	139951966573568 -> 139951966565616
	139951966573568 [label=AddBackward0]
	139951966575008 -> 139951966573568
	139951966575008 [label=NativeGroupNormBackward0]
	139951966574960 -> 139951966575008
	139951966574960 [label=ConvolutionBackward0]
	139951966575152 -> 139951966574960
	139951966575152 [label=ReluBackward0]
	139951966575296 -> 139951966575152
	139951966575296 [label=NativeGroupNormBackward0]
	139951966575392 -> 139951966575296
	139951966575392 [label=ConvolutionBackward0]
	139951966575584 -> 139951966575392
	139951966575584 [label=ReluBackward0]
	139951966575728 -> 139951966575584
	139951966575728 [label=NativeGroupNormBackward0]
	139951966575824 -> 139951966575728
	139951966575824 [label=ConvolutionBackward0]
	139951966574864 -> 139951966575824
	139951966574864 [label=ReluBackward0]
	139951966576112 -> 139951966574864
	139951966576112 [label=AddBackward0]
	139951966576208 -> 139951966576112
	139951966576208 [label=NativeGroupNormBackward0]
	139951966576352 -> 139951966576208
	139951966576352 [label=ConvolutionBackward0]
	139951966576544 -> 139951966576352
	139951966576544 [label=ReluBackward0]
	139951966576592 -> 139951966576544
	139951966576592 [label=NativeGroupNormBackward0]
	139951966609616 -> 139951966576592
	139951966609616 [label=ConvolutionBackward0]
	139951966609808 -> 139951966609616
	139951966609808 [label=ReluBackward0]
	139951966609952 -> 139951966609808
	139951966609952 [label=NativeGroupNormBackward0]
	139951966610048 -> 139951966609952
	139951966610048 [label=ConvolutionBackward0]
	139951966576160 -> 139951966610048
	139951966576160 [label=ReluBackward0]
	139951966610336 -> 139951966576160
	139951966610336 [label=AddBackward0]
	139951966610432 -> 139951966610336
	139951966610432 [label=NativeGroupNormBackward0]
	139951966610576 -> 139951966610432
	139951966610576 [label=ConvolutionBackward0]
	139951966610768 -> 139951966610576
	139951966610768 [label=ReluBackward0]
	139951966610912 -> 139951966610768
	139951966610912 [label=NativeGroupNormBackward0]
	139951966611008 -> 139951966610912
	139951966611008 [label=ConvolutionBackward0]
	139951966611200 -> 139951966611008
	139951966611200 [label=ReluBackward0]
	139951966611344 -> 139951966611200
	139951966611344 [label=NativeGroupNormBackward0]
	139951966611440 -> 139951966611344
	139951966611440 [label=ConvolutionBackward0]
	139951967004224 -> 139951966611440
	139951966611632 -> 139951966611440
	139951895752032 [label="Expert_Gate.expert_layers.1.layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	139951895752032 -> 139951966611632
	139951966611632 [label=AccumulateGrad]
	139951966611392 -> 139951966611344
	139951895752112 [label="Expert_Gate.expert_layers.1.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	139951895752112 -> 139951966611392
	139951966611392 [label=AccumulateGrad]
	139951966611248 -> 139951966611344
	139951895752192 [label="Expert_Gate.expert_layers.1.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	139951895752192 -> 139951966611248
	139951966611248 [label=AccumulateGrad]
	139951966611152 -> 139951966611008
	139951895752432 [label="Expert_Gate.expert_layers.1.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139951895752432 -> 139951966611152
	139951966611152 [label=AccumulateGrad]
	139951966610960 -> 139951966610912
	139951895752352 [label="Expert_Gate.expert_layers.1.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	139951895752352 -> 139951966610960
	139951966610960 [label=AccumulateGrad]
	139951966610816 -> 139951966610912
	139951895752512 [label="Expert_Gate.expert_layers.1.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	139951895752512 -> 139951966610816
	139951966610816 [label=AccumulateGrad]
	139951966610720 -> 139951966610576
	139951895937088 [label="Expert_Gate.expert_layers.1.layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951895937088 -> 139951966610720
	139951966610720 [label=AccumulateGrad]
	139951966610528 -> 139951966610432
	139951895937168 [label="Expert_Gate.expert_layers.1.layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	139951895937168 -> 139951966610528
	139951966610528 [label=AccumulateGrad]
	139951966610480 -> 139951966610432
	139951895937248 [label="Expert_Gate.expert_layers.1.layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	139951895937248 -> 139951966610480
	139951966610480 [label=AccumulateGrad]
	139951966610384 -> 139951966610336
	139951966610384 [label=NativeGroupNormBackward0]
	139951966611104 -> 139951966610384
	139951966611104 [label=ConvolutionBackward0]
	139951967004224 -> 139951966611104
	139951966611488 -> 139951966611104
	139951895751712 [label="Expert_Gate.expert_layers.1.layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	139951895751712 -> 139951966611488
	139951966611488 [label=AccumulateGrad]
	139951966610672 -> 139951966610384
	139951895751792 [label="Expert_Gate.expert_layers.1.layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	139951895751792 -> 139951966610672
	139951966610672 [label=AccumulateGrad]
	139951966610624 -> 139951966610384
	139951895751872 [label="Expert_Gate.expert_layers.1.layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	139951895751872 -> 139951966610624
	139951966610624 [label=AccumulateGrad]
	139951966610240 -> 139951966610048
	139951895937408 [label="Expert_Gate.expert_layers.1.layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139951895937408 -> 139951966610240
	139951966610240 [label=AccumulateGrad]
	139951966610000 -> 139951966609952
	139951895937488 [label="Expert_Gate.expert_layers.1.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	139951895937488 -> 139951966610000
	139951966610000 [label=AccumulateGrad]
	139951966609856 -> 139951966609952
	139951895937568 [label="Expert_Gate.expert_layers.1.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	139951895937568 -> 139951966609856
	139951966609856 [label=AccumulateGrad]
	139951966609760 -> 139951966609616
	139951895937808 [label="Expert_Gate.expert_layers.1.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139951895937808 -> 139951966609760
	139951966609760 [label=AccumulateGrad]
	139951966609568 -> 139951966576592
	139951895937728 [label="Expert_Gate.expert_layers.1.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	139951895937728 -> 139951966609568
	139951966609568 [label=AccumulateGrad]
	139951966609520 -> 139951966576592
	139951895937888 [label="Expert_Gate.expert_layers.1.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	139951895937888 -> 139951966609520
	139951966609520 [label=AccumulateGrad]
	139951966576496 -> 139951966576352
	139951895938048 [label="Expert_Gate.expert_layers.1.layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951895938048 -> 139951966576496
	139951966576496 [label=AccumulateGrad]
	139951966576304 -> 139951966576208
	139951895938128 [label="Expert_Gate.expert_layers.1.layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	139951895938128 -> 139951966576304
	139951966576304 [label=AccumulateGrad]
	139951966576256 -> 139951966576208
	139951895938208 [label="Expert_Gate.expert_layers.1.layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	139951895938208 -> 139951966576256
	139951966576256 [label=AccumulateGrad]
	139951966576160 -> 139951966576112
	139951966576016 -> 139951966575824
	139951895938368 [label="Expert_Gate.expert_layers.1.layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139951895938368 -> 139951966576016
	139951966576016 [label=AccumulateGrad]
	139951966575776 -> 139951966575728
	139951895938448 [label="Expert_Gate.expert_layers.1.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	139951895938448 -> 139951966575776
	139951966575776 [label=AccumulateGrad]
	139951966575632 -> 139951966575728
	139951895938528 [label="Expert_Gate.expert_layers.1.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	139951895938528 -> 139951966575632
	139951966575632 [label=AccumulateGrad]
	139951966575536 -> 139951966575392
	139951895938768 [label="Expert_Gate.expert_layers.1.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139951895938768 -> 139951966575536
	139951966575536 [label=AccumulateGrad]
	139951966575344 -> 139951966575296
	139951895938688 [label="Expert_Gate.expert_layers.1.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	139951895938688 -> 139951966575344
	139951966575344 [label=AccumulateGrad]
	139951966575200 -> 139951966575296
	139951895938848 [label="Expert_Gate.expert_layers.1.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	139951895938848 -> 139951966575200
	139951966575200 [label=AccumulateGrad]
	139951966575104 -> 139951966574960
	139951895939008 [label="Expert_Gate.expert_layers.1.layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951895939008 -> 139951966575104
	139951966575104 [label=AccumulateGrad]
	139951966574144 -> 139951966575008
	139951895939088 [label="Expert_Gate.expert_layers.1.layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	139951895939088 -> 139951966574144
	139951966574144 [label=AccumulateGrad]
	139951966574576 -> 139951966575008
	139951895939168 [label="Expert_Gate.expert_layers.1.layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	139951895939168 -> 139951966574576
	139951966574576 [label=AccumulateGrad]
	139951966574864 -> 139951966573568
	139951966574336 -> 139951966572752
	139951895939328 [label="Expert_Gate.expert_layers.1.layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139951895939328 -> 139951966574336
	139951966574336 [label=AccumulateGrad]
	139951966572944 -> 139951966568352
	139951895939408 [label="Expert_Gate.expert_layers.1.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	139951895939408 -> 139951966572944
	139951966572944 [label=AccumulateGrad]
	139951966572608 -> 139951966568352
	139951895939488 [label="Expert_Gate.expert_layers.1.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	139951895939488 -> 139951966572608
	139951966572608 [label=AccumulateGrad]
	139951966567632 -> 139951966567200
	139951895939728 [label="Expert_Gate.expert_layers.1.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139951895939728 -> 139951966567632
	139951966567632 [label=AccumulateGrad]
	139951966567392 -> 139951966567440
	139951895939648 [label="Expert_Gate.expert_layers.1.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	139951895939648 -> 139951966567392
	139951966567392 [label=AccumulateGrad]
	139951966566624 -> 139951966567440
	139951895939808 [label="Expert_Gate.expert_layers.1.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	139951895939808 -> 139951966566624
	139951966566624 [label=AccumulateGrad]
	139951966566960 -> 139951966566528
	139951895939968 [label="Expert_Gate.expert_layers.1.layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951895939968 -> 139951966566960
	139951966566960 [label=AccumulateGrad]
	139951966565808 -> 139951966566048
	139951895940048 [label="Expert_Gate.expert_layers.1.layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	139951895940048 -> 139951966565808
	139951966565808 [label=AccumulateGrad]
	139951966566000 -> 139951966566048
	139951895940128 [label="Expert_Gate.expert_layers.1.layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	139951895940128 -> 139951966566000
	139951966566000 [label=AccumulateGrad]
	139951966565616 -> 139951966565568
	139951966564992 -> 139951966564944
	139951966564992 [label=ReluBackward0]
	139951966566432 -> 139951966564992
	139951966566432 [label=AddBackward0]
	139951966567824 -> 139951966566432
	139951966567824 [label=NativeGroupNormBackward0]
	139951966567008 -> 139951966567824
	139951966567008 [label=ConvolutionBackward0]
	139951966573376 -> 139951966567008
	139951966573376 [label=ReluBackward0]
	139951966575488 -> 139951966573376
	139951966575488 [label=NativeGroupNormBackward0]
	139951966575248 -> 139951966575488
	139951966575248 [label=ConvolutionBackward0]
	139951966576400 -> 139951966575248
	139951966576400 [label=ReluBackward0]
	139951966576448 -> 139951966576400
	139951966576448 [label=NativeGroupNormBackward0]
	139951966575920 -> 139951966576448
	139951966575920 [label=ConvolutionBackward0]
	139951966566240 -> 139951966575920
	139951966566240 [label=ReluBackward0]
	139951966610144 -> 139951966566240
	139951966610144 [label=AddBackward0]
	139951966611584 -> 139951966610144
	139951966611584 [label=NativeGroupNormBackward0]
	139951966610864 -> 139951966611584
	139951966610864 [label=ConvolutionBackward0]
	139951966611824 -> 139951966610864
	139951966611824 [label=ReluBackward0]
	139951966611968 -> 139951966611824
	139951966611968 [label=NativeGroupNormBackward0]
	139951966612064 -> 139951966611968
	139951966612064 [label=ConvolutionBackward0]
	139951966612256 -> 139951966612064
	139951966612256 [label=ReluBackward0]
	139951966612400 -> 139951966612256
	139951966612400 [label=NativeGroupNormBackward0]
	139951966612496 -> 139951966612400
	139951966612496 [label=ConvolutionBackward0]
	139951966610288 -> 139951966612496
	139951966610288 [label=ReluBackward0]
	139951966612784 -> 139951966610288
	139951966612784 [label=AddBackward0]
	139951966612880 -> 139951966612784
	139951966612880 [label=NativeGroupNormBackward0]
	139951966613024 -> 139951966612880
	139951966613024 [label=ConvolutionBackward0]
	139951966613216 -> 139951966613024
	139951966613216 [label=ReluBackward0]
	139951966613360 -> 139951966613216
	139951966613360 [label=NativeGroupNormBackward0]
	139951966613456 -> 139951966613360
	139951966613456 [label=ConvolutionBackward0]
	139951966642384 -> 139951966613456
	139951966642384 [label=ReluBackward0]
	139951966642528 -> 139951966642384
	139951966642528 [label=NativeGroupNormBackward0]
	139951966642624 -> 139951966642528
	139951966642624 [label=ConvolutionBackward0]
	139951966612832 -> 139951966642624
	139951966612832 [label=ReluBackward0]
	139951966642912 -> 139951966612832
	139951966642912 [label=AddBackward0]
	139951966643008 -> 139951966642912
	139951966643008 [label=NativeGroupNormBackward0]
	139951966643152 -> 139951966643008
	139951966643152 [label=ConvolutionBackward0]
	139951966643344 -> 139951966643152
	139951966643344 [label=ReluBackward0]
	139951966643488 -> 139951966643344
	139951966643488 [label=NativeGroupNormBackward0]
	139951966643584 -> 139951966643488
	139951966643584 [label=ConvolutionBackward0]
	139951966643776 -> 139951966643584
	139951966643776 [label=ReluBackward0]
	139951966643920 -> 139951966643776
	139951966643920 [label=NativeGroupNormBackward0]
	139951966644016 -> 139951966643920
	139951966644016 [label=ConvolutionBackward0]
	139951967004176 -> 139951966644016
	139951966644208 -> 139951966644016
	139951823225296 [label="Expert_Gate.expert_layers.2.layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	139951823225296 -> 139951966644208
	139951966644208 [label=AccumulateGrad]
	139951966643968 -> 139951966643920
	139951823225376 [label="Expert_Gate.expert_layers.2.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	139951823225376 -> 139951966643968
	139951966643968 [label=AccumulateGrad]
	139951966643824 -> 139951966643920
	139951823225456 [label="Expert_Gate.expert_layers.2.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	139951823225456 -> 139951966643824
	139951966643824 [label=AccumulateGrad]
	139951966643728 -> 139951966643584
	139951823225696 [label="Expert_Gate.expert_layers.2.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139951823225696 -> 139951966643728
	139951966643728 [label=AccumulateGrad]
	139951966643536 -> 139951966643488
	139951823225616 [label="Expert_Gate.expert_layers.2.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	139951823225616 -> 139951966643536
	139951966643536 [label=AccumulateGrad]
	139951966643392 -> 139951966643488
	139951823225776 [label="Expert_Gate.expert_layers.2.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	139951823225776 -> 139951966643392
	139951966643392 [label=AccumulateGrad]
	139951966643296 -> 139951966643152
	139951823225936 [label="Expert_Gate.expert_layers.2.layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951823225936 -> 139951966643296
	139951966643296 [label=AccumulateGrad]
	139951966643104 -> 139951966643008
	139951823226016 [label="Expert_Gate.expert_layers.2.layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	139951823226016 -> 139951966643104
	139951966643104 [label=AccumulateGrad]
	139951966643056 -> 139951966643008
	139951823226096 [label="Expert_Gate.expert_layers.2.layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	139951823226096 -> 139951966643056
	139951966643056 [label=AccumulateGrad]
	139951966642960 -> 139951966642912
	139951966642960 [label=NativeGroupNormBackward0]
	139951966643680 -> 139951966642960
	139951966643680 [label=ConvolutionBackward0]
	139951967004176 -> 139951966643680
	139951966644064 -> 139951966643680
	139951823224976 [label="Expert_Gate.expert_layers.2.layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	139951823224976 -> 139951966644064
	139951966644064 [label=AccumulateGrad]
	139951966643248 -> 139951966642960
	139951823225056 [label="Expert_Gate.expert_layers.2.layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	139951823225056 -> 139951966643248
	139951966643248 [label=AccumulateGrad]
	139951966643200 -> 139951966642960
	139951823225136 [label="Expert_Gate.expert_layers.2.layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	139951823225136 -> 139951966643200
	139951966643200 [label=AccumulateGrad]
	139951966642816 -> 139951966642624
	139951823226256 [label="Expert_Gate.expert_layers.2.layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139951823226256 -> 139951966642816
	139951966642816 [label=AccumulateGrad]
	139951966642576 -> 139951966642528
	139951823226336 [label="Expert_Gate.expert_layers.2.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	139951823226336 -> 139951966642576
	139951966642576 [label=AccumulateGrad]
	139951966642432 -> 139951966642528
	139951823226416 [label="Expert_Gate.expert_layers.2.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	139951823226416 -> 139951966642432
	139951966642432 [label=AccumulateGrad]
	139951966642336 -> 139951966613456
	139951823226656 [label="Expert_Gate.expert_layers.2.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139951823226656 -> 139951966642336
	139951966642336 [label=AccumulateGrad]
	139951966613408 -> 139951966613360
	139951823226576 [label="Expert_Gate.expert_layers.2.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	139951823226576 -> 139951966613408
	139951966613408 [label=AccumulateGrad]
	139951966613264 -> 139951966613360
	139951823226736 [label="Expert_Gate.expert_layers.2.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	139951823226736 -> 139951966613264
	139951966613264 [label=AccumulateGrad]
	139951966613168 -> 139951966613024
	139951823226896 [label="Expert_Gate.expert_layers.2.layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951823226896 -> 139951966613168
	139951966613168 [label=AccumulateGrad]
	139951966612976 -> 139951966612880
	139951823226976 [label="Expert_Gate.expert_layers.2.layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	139951823226976 -> 139951966612976
	139951966612976 [label=AccumulateGrad]
	139951966612928 -> 139951966612880
	139951823227056 [label="Expert_Gate.expert_layers.2.layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	139951823227056 -> 139951966612928
	139951966612928 [label=AccumulateGrad]
	139951966612832 -> 139951966612784
	139951966612688 -> 139951966612496
	139951823227216 [label="Expert_Gate.expert_layers.2.layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139951823227216 -> 139951966612688
	139951966612688 [label=AccumulateGrad]
	139951966612448 -> 139951966612400
	139951823227296 [label="Expert_Gate.expert_layers.2.layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	139951823227296 -> 139951966612448
	139951966612448 [label=AccumulateGrad]
	139951966612304 -> 139951966612400
	139951823227376 [label="Expert_Gate.expert_layers.2.layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	139951823227376 -> 139951966612304
	139951966612304 [label=AccumulateGrad]
	139951966612208 -> 139951966612064
	139951823227616 [label="Expert_Gate.expert_layers.2.layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139951823227616 -> 139951966612208
	139951966612208 [label=AccumulateGrad]
	139951966612016 -> 139951966611968
	139951823227536 [label="Expert_Gate.expert_layers.2.layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	139951823227536 -> 139951966612016
	139951966612016 [label=AccumulateGrad]
	139951966611872 -> 139951966611968
	139951823227696 [label="Expert_Gate.expert_layers.2.layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	139951823227696 -> 139951966611872
	139951966611872 [label=AccumulateGrad]
	139951966611776 -> 139951966610864
	139951823227856 [label="Expert_Gate.expert_layers.2.layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951823227856 -> 139951966611776
	139951966611776 [label=AccumulateGrad]
	139951966611296 -> 139951966611584
	139951823227936 [label="Expert_Gate.expert_layers.2.layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	139951823227936 -> 139951966611296
	139951966611296 [label=AccumulateGrad]
	139951966611728 -> 139951966611584
	139951823228016 [label="Expert_Gate.expert_layers.2.layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	139951823228016 -> 139951966611728
	139951966611728 [label=AccumulateGrad]
	139951966610288 -> 139951966610144
	139951966609904 -> 139951966575920
	139951823228176 [label="Expert_Gate.expert_layers.2.layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	139951823228176 -> 139951966609904
	139951966609904 [label=AccumulateGrad]
	139951966609664 -> 139951966576448
	139951823228256 [label="Expert_Gate.expert_layers.2.layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	139951823228256 -> 139951966609664
	139951966609664 [label=AccumulateGrad]
	139951966609712 -> 139951966576448
	139951823228336 [label="Expert_Gate.expert_layers.2.layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	139951823228336 -> 139951966609712
	139951966609712 [label=AccumulateGrad]
	139951966575680 -> 139951966575248
	139951823228576 [label="Expert_Gate.expert_layers.2.layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139951823228576 -> 139951966575680
	139951966575680 [label=AccumulateGrad]
	139951966575440 -> 139951966575488
	139951823228496 [label="Expert_Gate.expert_layers.2.layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	139951823228496 -> 139951966575440
	139951966575440 [label=AccumulateGrad]
	139951966573424 -> 139951966575488
	139951823228656 [label="Expert_Gate.expert_layers.2.layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	139951823228656 -> 139951966573424
	139951966573424 [label=AccumulateGrad]
	139951966574816 -> 139951966567008
	139951823228816 [label="Expert_Gate.expert_layers.2.layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	139951823228816 -> 139951966574816
	139951966574816 [label=AccumulateGrad]
	139951966568016 -> 139951966567824
	139951805034560 [label="Expert_Gate.expert_layers.2.layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	139951805034560 -> 139951966568016
	139951966568016 [label=AccumulateGrad]
	139951966573184 -> 139951966567824
	139951805034640 [label="Expert_Gate.expert_layers.2.layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	139951805034640 -> 139951966573184
	139951966573184 [label=AccumulateGrad]
	139951966566240 -> 139951966566432
	139951966564896 -> 139951966564752
	139951966564896 [label=AddBackward0]
	139951966565184 -> 139951966564896
	139951966565184 [label=TanhBackward0]
	139951966565136 -> 139951966565184
	139951966565136 [label=AddBackward0]
	139951966575968 -> 139951966565136
	139951966575968 [label=MulBackward0]
	139951966576064 -> 139951966575968
	139951966576064 [label=MulBackward0]
	139951966610192 -> 139951966576064
	139951966610192 [label=PowBackward0]
	139951966612160 -> 139951966610192
	139951966612160 [label=AddBackward0]
	139951966611920 -> 139951966612160
	139951966611920 [label=SumBackward1]
	139951966612544 -> 139951966611920
	139951966612544 [label=PowBackward0]
	139951966564944 -> 139951966612544
	139951966611680 -> 139951966576064
	139951798052800 [label="Expert_Gate.fusion_modules.2.alpha
 (1, 1536, 1, 1)" fillcolor=lightblue]
	139951798052800 -> 139951966611680
	139951966611680 [label=AccumulateGrad]
	139951966575056 -> 139951966575968
	139951966575056 [label=DivBackward0]
	139951966612112 -> 139951966575056
	139951798052720 [label="Expert_Gate.fusion_modules.2.gamma
 (1, 1536, 1, 1)" fillcolor=lightblue]
	139951798052720 -> 139951966612112
	139951966612112 [label=AccumulateGrad]
	139951966612640 -> 139951966575056
	139951966612640 [label=PowBackward0]
	139951966611536 -> 139951966612640
	139951966611536 [label=AddBackward0]
	139951966612592 -> 139951966611536
	139951966612592 [label=MeanBackward1]
	139951966613120 -> 139951966612592
	139951966613120 [label=PowBackward0]
	139951966576064 -> 139951966613120
	139951966575872 -> 139951966565136
	139951798054720 [label="Expert_Gate.fusion_modules.2.beta
 (1, 1536, 1, 1)" fillcolor=lightblue]
	139951798054720 -> 139951966575872
	139951966575872 [label=AccumulateGrad]
	139951966564704 -> 139951966564560
	139951746236800 [label="Expert_Gate.fusion_modules.2.conv.weight
 (512, 1536, 1, 1)" fillcolor=lightblue]
	139951746236800 -> 139951966564704
	139951966564704 [label=AccumulateGrad]
	139951966564656 -> 139951966564560
	139951746236880 [label="Expert_Gate.fusion_modules.2.conv.bias
 (512)" fillcolor=lightblue]
	139951746236880 -> 139951966564656
	139951966564656 [label=AccumulateGrad]
	139951966564512 -> 139951966527440
	139951746236960 [label="Expert_Gate.fusion_modules.2.bn.weight
 (512)" fillcolor=lightblue]
	139951746236960 -> 139951966564512
	139951966564512 [label=AccumulateGrad]
	139951966564416 -> 139951966527440
	139951746237040 [label="Expert_Gate.fusion_modules.2.bn.bias
 (512)" fillcolor=lightblue]
	139951746237040 -> 139951966564416
	139951966564416 [label=AccumulateGrad]
	139951966527344 -> 139951966525520
	139951966527344 [label=UpsampleBilinear2DBackward0]
	139951966564800 -> 139951966527344
	139951966564800 [label=AddBackward0]
	139951966565232 -> 139951966564800
	139951966565232 [label=ReluBackward0]
	139951966573472 -> 139951966565232
	139951966573472 [label=ConvolutionBackward0]
	139951966610096 -> 139951966573472
	139951966610096 [label=ReluBackward0]
	139951966613312 -> 139951966610096
	139951966613312 [label=ConvolutionBackward0]
	139951966612352 -> 139951966613312
	139951966612352 [label=CatBackward0]
	139951966642672 -> 139951966612352
	139951966642672 [label=ReluBackward0]
	139951966642720 -> 139951966642672
	139951966642720 [label=NativeGroupNormBackward0]
	139951966644160 -> 139951966642720
	139951966644160 [label=ConvolutionBackward0]
	139951966644256 -> 139951966644160
	139951966644256 [label=MulBackward0]
	139951966644448 -> 139951966644256
	139951966644448 [label=CatBackward0]
	139951966644592 -> 139951966644448
	139951966644592 [label=ReluBackward0]
	139951966644784 -> 139951966644592
	139951966644784 [label=AddBackward0]
	139951966644880 -> 139951966644784
	139951966644880 [label=NativeGroupNormBackward0]
	139951966645024 -> 139951966644880
	139951966645024 [label=ConvolutionBackward0]
	139951966645216 -> 139951966645024
	139951966645216 [label=ReluBackward0]
	139951966645360 -> 139951966645216
	139951966645360 [label=NativeGroupNormBackward0]
	139951966645456 -> 139951966645360
	139951966645456 [label=ConvolutionBackward0]
	139951966645648 -> 139951966645456
	139951966645648 [label=ReluBackward0]
	139951966645792 -> 139951966645648
	139951966645792 [label=NativeGroupNormBackward0]
	139951966645888 -> 139951966645792
	139951966645888 [label=ConvolutionBackward0]
	139951966644832 -> 139951966645888
	139951966644832 [label=ReluBackward0]
	139951966646176 -> 139951966644832
	139951966646176 [label=AddBackward0]
	139951966646224 -> 139951966646176
	139951966646224 [label=NativeGroupNormBackward0]
	139951966679248 -> 139951966646224
	139951966679248 [label=ConvolutionBackward0]
	139951966679440 -> 139951966679248
	139951966679440 [label=ReluBackward0]
	139951966679584 -> 139951966679440
	139951966679584 [label=NativeGroupNormBackward0]
	139951966679680 -> 139951966679584
	139951966679680 [label=ConvolutionBackward0]
	139951966679872 -> 139951966679680
	139951966679872 [label=ReluBackward0]
	139951966680016 -> 139951966679872
	139951966680016 [label=NativeGroupNormBackward0]
	139951966680112 -> 139951966680016
	139951966680112 [label=ConvolutionBackward0]
	139951966645984 -> 139951966680112
	139951966645984 [label=ReluBackward0]
	139951966680400 -> 139951966645984
	139951966680400 [label=AddBackward0]
	139951966680496 -> 139951966680400
	139951966680496 [label=NativeGroupNormBackward0]
	139951966680640 -> 139951966680496
	139951966680640 [label=ConvolutionBackward0]
	139951966680832 -> 139951966680640
	139951966680832 [label=ReluBackward0]
	139951966680976 -> 139951966680832
	139951966680976 [label=NativeGroupNormBackward0]
	139951966681072 -> 139951966680976
	139951966681072 [label=ConvolutionBackward0]
	139951966681264 -> 139951966681072
	139951966681264 [label=ReluBackward0]
	139951966681408 -> 139951966681264
	139951966681408 [label=NativeGroupNormBackward0]
	139951966681504 -> 139951966681408
	139951966681504 [label=ConvolutionBackward0]
	139951966680448 -> 139951966681504
	139951966680448 [label=ReluBackward0]
	139951966681792 -> 139951966680448
	139951966681792 [label=AddBackward0]
	139951966681888 -> 139951966681792
	139951966681888 [label=NativeGroupNormBackward0]
	139951966682032 -> 139951966681888
	139951966682032 [label=ConvolutionBackward0]
	139951966682224 -> 139951966682032
	139951966682224 [label=ReluBackward0]
	139951966682368 -> 139951966682224
	139951966682368 [label=NativeGroupNormBackward0]
	139951966682464 -> 139951966682368
	139951966682464 [label=ConvolutionBackward0]
	139951966682656 -> 139951966682464
	139951966682656 [label=ReluBackward0]
	139951966682800 -> 139951966682656
	139951966682800 [label=NativeGroupNormBackward0]
	139951966682896 -> 139951966682800
	139951966682896 [label=ConvolutionBackward0]
	139951966681840 -> 139951966682896
	139951966681840 [label=ReluBackward0]
	139951966682992 -> 139951966681840
	139951966682992 [label=AddBackward0]
	139951966691536 -> 139951966682992
	139951966691536 [label=NativeGroupNormBackward0]
	139951966691680 -> 139951966691536
	139951966691680 [label=ConvolutionBackward0]
	139951966691872 -> 139951966691680
	139951966691872 [label=ReluBackward0]
	139951966692016 -> 139951966691872
	139951966692016 [label=NativeGroupNormBackward0]
	139951966692112 -> 139951966692016
	139951966692112 [label=ConvolutionBackward0]
	139951966692304 -> 139951966692112
	139951966692304 [label=ReluBackward0]
	139951966692448 -> 139951966692304
	139951966692448 [label=NativeGroupNormBackward0]
	139951966692544 -> 139951966692448
	139951966692544 [label=ConvolutionBackward0]
	139951966691488 -> 139951966692544
	139951966691488 [label=ReluBackward0]
	139951966692832 -> 139951966691488
	139951966692832 [label=AddBackward0]
	139951966692928 -> 139951966692832
	139951966692928 [label=NativeGroupNormBackward0]
	139951966693072 -> 139951966692928
	139951966693072 [label=ConvolutionBackward0]
	139951966693264 -> 139951966693072
	139951966693264 [label=ReluBackward0]
	139951966693408 -> 139951966693264
	139951966693408 [label=NativeGroupNormBackward0]
	139951966693504 -> 139951966693408
	139951966693504 [label=ConvolutionBackward0]
	139951966693696 -> 139951966693504
	139951966693696 [label=ReluBackward0]
	139951966693840 -> 139951966693696
	139951966693840 [label=NativeGroupNormBackward0]
	139951966693936 -> 139951966693840
	139951966693936 [label=ConvolutionBackward0]
	139951966565088 -> 139951966693936
	139951966694128 -> 139951966693936
	139951958694464 [label="Expert_Gate.expert_layers.0.layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	139951958694464 -> 139951966694128
	139951966694128 [label=AccumulateGrad]
	139951966693888 -> 139951966693840
	139951958694544 [label="Expert_Gate.expert_layers.0.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	139951958694544 -> 139951966693888
	139951966693888 [label=AccumulateGrad]
	139951966693744 -> 139951966693840
	139951958694624 [label="Expert_Gate.expert_layers.0.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	139951958694624 -> 139951966693744
	139951966693744 [label=AccumulateGrad]
	139951966693648 -> 139951966693504
	139951958694864 [label="Expert_Gate.expert_layers.0.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951958694864 -> 139951966693648
	139951966693648 [label=AccumulateGrad]
	139951966693456 -> 139951966693408
	139951958694784 [label="Expert_Gate.expert_layers.0.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	139951958694784 -> 139951966693456
	139951966693456 [label=AccumulateGrad]
	139951966693312 -> 139951966693408
	139951958694944 [label="Expert_Gate.expert_layers.0.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	139951958694944 -> 139951966693312
	139951966693312 [label=AccumulateGrad]
	139951966693216 -> 139951966693072
	139951958695104 [label="Expert_Gate.expert_layers.0.layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951958695104 -> 139951966693216
	139951966693216 [label=AccumulateGrad]
	139951966693024 -> 139951966692928
	139951958695184 [label="Expert_Gate.expert_layers.0.layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	139951958695184 -> 139951966693024
	139951966693024 [label=AccumulateGrad]
	139951966692976 -> 139951966692928
	139951958695264 [label="Expert_Gate.expert_layers.0.layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	139951958695264 -> 139951966692976
	139951966692976 [label=AccumulateGrad]
	139951966692880 -> 139951966692832
	139951966692880 [label=NativeGroupNormBackward0]
	139951966693600 -> 139951966692880
	139951966693600 [label=ConvolutionBackward0]
	139951966565088 -> 139951966693600
	139951966693984 -> 139951966693600
	139951958694144 [label="Expert_Gate.expert_layers.0.layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	139951958694144 -> 139951966693984
	139951966693984 [label=AccumulateGrad]
	139951966693168 -> 139951966692880
	139951958694224 [label="Expert_Gate.expert_layers.0.layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	139951958694224 -> 139951966693168
	139951966693168 [label=AccumulateGrad]
	139951966693120 -> 139951966692880
	139951958694304 [label="Expert_Gate.expert_layers.0.layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	139951958694304 -> 139951966693120
	139951966693120 [label=AccumulateGrad]
	139951966692736 -> 139951966692544
	139951958695424 [label="Expert_Gate.expert_layers.0.layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951958695424 -> 139951966692736
	139951966692736 [label=AccumulateGrad]
	139951966692496 -> 139951966692448
	139951958695504 [label="Expert_Gate.expert_layers.0.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	139951958695504 -> 139951966692496
	139951966692496 [label=AccumulateGrad]
	139951966692352 -> 139951966692448
	139951958695584 [label="Expert_Gate.expert_layers.0.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	139951958695584 -> 139951966692352
	139951966692352 [label=AccumulateGrad]
	139951966692256 -> 139951966692112
	139951958695824 [label="Expert_Gate.expert_layers.0.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951958695824 -> 139951966692256
	139951966692256 [label=AccumulateGrad]
	139951966692064 -> 139951966692016
	139951958695744 [label="Expert_Gate.expert_layers.0.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	139951958695744 -> 139951966692064
	139951966692064 [label=AccumulateGrad]
	139951966691920 -> 139951966692016
	139951958872128 [label="Expert_Gate.expert_layers.0.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	139951958872128 -> 139951966691920
	139951966691920 [label=AccumulateGrad]
	139951966691824 -> 139951966691680
	139951958872288 [label="Expert_Gate.expert_layers.0.layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951958872288 -> 139951966691824
	139951966691824 [label=AccumulateGrad]
	139951966691632 -> 139951966691536
	139951958872368 [label="Expert_Gate.expert_layers.0.layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	139951958872368 -> 139951966691632
	139951966691632 [label=AccumulateGrad]
	139951966691584 -> 139951966691536
	139951958872448 [label="Expert_Gate.expert_layers.0.layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	139951958872448 -> 139951966691584
	139951966691584 [label=AccumulateGrad]
	139951966691488 -> 139951966682992
	139951966683088 -> 139951966682896
	139951958872608 [label="Expert_Gate.expert_layers.0.layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951958872608 -> 139951966683088
	139951966683088 [label=AccumulateGrad]
	139951966682848 -> 139951966682800
	139951958872688 [label="Expert_Gate.expert_layers.0.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	139951958872688 -> 139951966682848
	139951966682848 [label=AccumulateGrad]
	139951966682704 -> 139951966682800
	139951958872768 [label="Expert_Gate.expert_layers.0.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	139951958872768 -> 139951966682704
	139951966682704 [label=AccumulateGrad]
	139951966682608 -> 139951966682464
	139951958873008 [label="Expert_Gate.expert_layers.0.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951958873008 -> 139951966682608
	139951966682608 [label=AccumulateGrad]
	139951966682416 -> 139951966682368
	139951958872928 [label="Expert_Gate.expert_layers.0.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	139951958872928 -> 139951966682416
	139951966682416 [label=AccumulateGrad]
	139951966682272 -> 139951966682368
	139951958873088 [label="Expert_Gate.expert_layers.0.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	139951958873088 -> 139951966682272
	139951966682272 [label=AccumulateGrad]
	139951966682176 -> 139951966682032
	139951958873248 [label="Expert_Gate.expert_layers.0.layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951958873248 -> 139951966682176
	139951966682176 [label=AccumulateGrad]
	139951966681984 -> 139951966681888
	139951958873328 [label="Expert_Gate.expert_layers.0.layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	139951958873328 -> 139951966681984
	139951966681984 [label=AccumulateGrad]
	139951966681936 -> 139951966681888
	139951958873408 [label="Expert_Gate.expert_layers.0.layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	139951958873408 -> 139951966681936
	139951966681936 [label=AccumulateGrad]
	139951966681840 -> 139951966681792
	139951966681696 -> 139951966681504
	139951958873568 [label="Expert_Gate.expert_layers.0.layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951958873568 -> 139951966681696
	139951966681696 [label=AccumulateGrad]
	139951966681456 -> 139951966681408
	139951958873648 [label="Expert_Gate.expert_layers.0.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	139951958873648 -> 139951966681456
	139951966681456 [label=AccumulateGrad]
	139951966681312 -> 139951966681408
	139951958873728 [label="Expert_Gate.expert_layers.0.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	139951958873728 -> 139951966681312
	139951966681312 [label=AccumulateGrad]
	139951966681216 -> 139951966681072
	139951958873968 [label="Expert_Gate.expert_layers.0.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951958873968 -> 139951966681216
	139951966681216 [label=AccumulateGrad]
	139951966681024 -> 139951966680976
	139951958873888 [label="Expert_Gate.expert_layers.0.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	139951958873888 -> 139951966681024
	139951966681024 [label=AccumulateGrad]
	139951966680880 -> 139951966680976
	139951958874048 [label="Expert_Gate.expert_layers.0.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	139951958874048 -> 139951966680880
	139951966680880 [label=AccumulateGrad]
	139951966680784 -> 139951966680640
	139951958874208 [label="Expert_Gate.expert_layers.0.layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951958874208 -> 139951966680784
	139951966680784 [label=AccumulateGrad]
	139951966680592 -> 139951966680496
	139951958874288 [label="Expert_Gate.expert_layers.0.layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	139951958874288 -> 139951966680592
	139951966680592 [label=AccumulateGrad]
	139951966680544 -> 139951966680496
	139951958874368 [label="Expert_Gate.expert_layers.0.layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	139951958874368 -> 139951966680544
	139951966680544 [label=AccumulateGrad]
	139951966680448 -> 139951966680400
	139951966680304 -> 139951966680112
	139951958874528 [label="Expert_Gate.expert_layers.0.layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951958874528 -> 139951966680304
	139951966680304 [label=AccumulateGrad]
	139951966680064 -> 139951966680016
	139951958874608 [label="Expert_Gate.expert_layers.0.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	139951958874608 -> 139951966680064
	139951966680064 [label=AccumulateGrad]
	139951966679920 -> 139951966680016
	139951958874688 [label="Expert_Gate.expert_layers.0.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	139951958874688 -> 139951966679920
	139951966679920 [label=AccumulateGrad]
	139951966679824 -> 139951966679680
	139951958874928 [label="Expert_Gate.expert_layers.0.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951958874928 -> 139951966679824
	139951966679824 [label=AccumulateGrad]
	139951966679632 -> 139951966679584
	139951958874848 [label="Expert_Gate.expert_layers.0.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	139951958874848 -> 139951966679632
	139951966679632 [label=AccumulateGrad]
	139951966679488 -> 139951966679584
	139951958875008 [label="Expert_Gate.expert_layers.0.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	139951958875008 -> 139951966679488
	139951966679488 [label=AccumulateGrad]
	139951966679392 -> 139951966679248
	139951958875168 [label="Expert_Gate.expert_layers.0.layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951958875168 -> 139951966679392
	139951966679392 [label=AccumulateGrad]
	139951966679200 -> 139951966646224
	139951958875248 [label="Expert_Gate.expert_layers.0.layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	139951958875248 -> 139951966679200
	139951966679200 [label=AccumulateGrad]
	139951966679152 -> 139951966646224
	139951958875328 [label="Expert_Gate.expert_layers.0.layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	139951958875328 -> 139951966679152
	139951966679152 [label=AccumulateGrad]
	139951966645984 -> 139951966646176
	139951966646080 -> 139951966645888
	139951958875488 [label="Expert_Gate.expert_layers.0.layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951958875488 -> 139951966646080
	139951966646080 [label=AccumulateGrad]
	139951966645840 -> 139951966645792
	139951958875568 [label="Expert_Gate.expert_layers.0.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	139951958875568 -> 139951966645840
	139951966645840 [label=AccumulateGrad]
	139951966645696 -> 139951966645792
	139951958875648 [label="Expert_Gate.expert_layers.0.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	139951958875648 -> 139951966645696
	139951966645696 [label=AccumulateGrad]
	139951966645600 -> 139951966645456
	139951958875888 [label="Expert_Gate.expert_layers.0.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951958875888 -> 139951966645600
	139951966645600 [label=AccumulateGrad]
	139951966645408 -> 139951966645360
	139951958875808 [label="Expert_Gate.expert_layers.0.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	139951958875808 -> 139951966645408
	139951966645408 [label=AccumulateGrad]
	139951966645264 -> 139951966645360
	139951958875968 [label="Expert_Gate.expert_layers.0.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	139951958875968 -> 139951966645264
	139951966645264 [label=AccumulateGrad]
	139951966645168 -> 139951966645024
	139951949328448 [label="Expert_Gate.expert_layers.0.layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951949328448 -> 139951966645168
	139951966645168 [label=AccumulateGrad]
	139951966644976 -> 139951966644880
	139951949328528 [label="Expert_Gate.expert_layers.0.layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	139951949328528 -> 139951966644976
	139951966644976 [label=AccumulateGrad]
	139951966644928 -> 139951966644880
	139951949328608 [label="Expert_Gate.expert_layers.0.layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	139951949328608 -> 139951966644928
	139951966644928 [label=AccumulateGrad]
	139951966644832 -> 139951966644784
	139951966644544 -> 139951966644448
	139951966644544 [label=ReluBackward0]
	139951966645072 -> 139951966644544
	139951966645072 [label=AddBackward0]
	139951966645552 -> 139951966645072
	139951966645552 [label=NativeGroupNormBackward0]
	139951966646032 -> 139951966645552
	139951966646032 [label=ConvolutionBackward0]
	139951966646128 -> 139951966646032
	139951966646128 [label=ReluBackward0]
	139951966679776 -> 139951966646128
	139951966679776 [label=NativeGroupNormBackward0]
	139951966679536 -> 139951966679776
	139951966679536 [label=ConvolutionBackward0]
	139951966680688 -> 139951966679536
	139951966680688 [label=ReluBackward0]
	139951966680736 -> 139951966680688
	139951966680736 [label=NativeGroupNormBackward0]
	139951966681120 -> 139951966680736
	139951966681120 [label=ConvolutionBackward0]
	139951966645120 -> 139951966681120
	139951966645120 [label=ReluBackward0]
	139951966681600 -> 139951966645120
	139951966681600 [label=AddBackward0]
	139951966682128 -> 139951966681600
	139951966682128 [label=NativeGroupNormBackward0]
	139951966682320 -> 139951966682128
	139951966682320 [label=ConvolutionBackward0]
	139951966682752 -> 139951966682320
	139951966682752 [label=ReluBackward0]
	139951966691776 -> 139951966682752
	139951966691776 [label=NativeGroupNormBackward0]
	139951966692160 -> 139951966691776
	139951966692160 [label=ConvolutionBackward0]
	139951966692400 -> 139951966692160
	139951966692400 [label=ReluBackward0]
	139951966692784 -> 139951966692400
	139951966692784 [label=NativeGroupNormBackward0]
	139951966694224 -> 139951966692784
	139951966694224 [label=ConvolutionBackward0]
	139951966681744 -> 139951966694224
	139951966681744 [label=ReluBackward0]
	139951966694320 -> 139951966681744
	139951966694320 [label=AddBackward0]
	139951966694416 -> 139951966694320
	139951966694416 [label=NativeGroupNormBackward0]
	139951966694560 -> 139951966694416
	139951966694560 [label=ConvolutionBackward0]
	139951966694752 -> 139951966694560
	139951966694752 [label=ReluBackward0]
	139951966694896 -> 139951966694752
	139951966694896 [label=NativeGroupNormBackward0]
	139951966694992 -> 139951966694896
	139951966694992 [label=ConvolutionBackward0]
	139951966695184 -> 139951966694992
	139951966695184 [label=ReluBackward0]
	139951966695328 -> 139951966695184
	139951966695328 [label=NativeGroupNormBackward0]
	139951966695376 -> 139951966695328
	139951966695376 [label=ConvolutionBackward0]
	139951966694368 -> 139951966695376
	139951966694368 [label=ReluBackward0]
	139951966732640 -> 139951966694368
	139951966732640 [label=AddBackward0]
	139951966732736 -> 139951966732640
	139951966732736 [label=NativeGroupNormBackward0]
	139951966732880 -> 139951966732736
	139951966732880 [label=ConvolutionBackward0]
	139951966733072 -> 139951966732880
	139951966733072 [label=ReluBackward0]
	139951966733216 -> 139951966733072
	139951966733216 [label=NativeGroupNormBackward0]
	139951966733312 -> 139951966733216
	139951966733312 [label=ConvolutionBackward0]
	139951966733504 -> 139951966733312
	139951966733504 [label=ReluBackward0]
	139951966733648 -> 139951966733504
	139951966733648 [label=NativeGroupNormBackward0]
	139951966733744 -> 139951966733648
	139951966733744 [label=ConvolutionBackward0]
	139951966732688 -> 139951966733744
	139951966732688 [label=ReluBackward0]
	139951966734032 -> 139951966732688
	139951966734032 [label=AddBackward0]
	139951966734128 -> 139951966734032
	139951966734128 [label=NativeGroupNormBackward0]
	139951966734272 -> 139951966734128
	139951966734272 [label=ConvolutionBackward0]
	139951966734464 -> 139951966734272
	139951966734464 [label=ReluBackward0]
	139951966734608 -> 139951966734464
	139951966734608 [label=NativeGroupNormBackward0]
	139951966734704 -> 139951966734608
	139951966734704 [label=ConvolutionBackward0]
	139951966734896 -> 139951966734704
	139951966734896 [label=ReluBackward0]
	139951966735040 -> 139951966734896
	139951966735040 [label=NativeGroupNormBackward0]
	139951966735136 -> 139951966735040
	139951966735136 [label=ConvolutionBackward0]
	139951966734080 -> 139951966735136
	139951966734080 [label=ReluBackward0]
	139951966735424 -> 139951966734080
	139951966735424 [label=AddBackward0]
	139951966735520 -> 139951966735424
	139951966735520 [label=NativeGroupNormBackward0]
	139951966735664 -> 139951966735520
	139951966735664 [label=ConvolutionBackward0]
	139951966735856 -> 139951966735664
	139951966735856 [label=ReluBackward0]
	139951966736000 -> 139951966735856
	139951966736000 [label=NativeGroupNormBackward0]
	139951966736096 -> 139951966736000
	139951966736096 [label=ConvolutionBackward0]
	139951966736288 -> 139951966736096
	139951966736288 [label=ReluBackward0]
	139951966736336 -> 139951966736288
	139951966736336 [label=NativeGroupNormBackward0]
	139951966740688 -> 139951966736336
	139951966740688 [label=ConvolutionBackward0]
	139951966565040 -> 139951966740688
	139951966740880 -> 139951966740688
	139951895940608 [label="Expert_Gate.expert_layers.1.layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	139951895940608 -> 139951966740880
	139951966740880 [label=AccumulateGrad]
	139951966740640 -> 139951966736336
	139951895940688 [label="Expert_Gate.expert_layers.1.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	139951895940688 -> 139951966740640
	139951966740640 [label=AccumulateGrad]
	139951966740544 -> 139951966736336
	139951895940768 [label="Expert_Gate.expert_layers.1.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	139951895940768 -> 139951966740544
	139951966740544 [label=AccumulateGrad]
	139951966736240 -> 139951966736096
	139951895941008 [label="Expert_Gate.expert_layers.1.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951895941008 -> 139951966736240
	139951966736240 [label=AccumulateGrad]
	139951966736048 -> 139951966736000
	139951895940928 [label="Expert_Gate.expert_layers.1.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	139951895940928 -> 139951966736048
	139951966736048 [label=AccumulateGrad]
	139951966735904 -> 139951966736000
	139951881965632 [label="Expert_Gate.expert_layers.1.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	139951881965632 -> 139951966735904
	139951966735904 [label=AccumulateGrad]
	139951966735808 -> 139951966735664
	139951881965792 [label="Expert_Gate.expert_layers.1.layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951881965792 -> 139951966735808
	139951966735808 [label=AccumulateGrad]
	139951966735616 -> 139951966735520
	139951881965872 [label="Expert_Gate.expert_layers.1.layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	139951881965872 -> 139951966735616
	139951966735616 [label=AccumulateGrad]
	139951966735568 -> 139951966735520
	139951881965952 [label="Expert_Gate.expert_layers.1.layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	139951881965952 -> 139951966735568
	139951966735568 [label=AccumulateGrad]
	139951966735472 -> 139951966735424
	139951966735472 [label=NativeGroupNormBackward0]
	139951966736192 -> 139951966735472
	139951966736192 [label=ConvolutionBackward0]
	139951966565040 -> 139951966736192
	139951966735952 -> 139951966736192
	139951895940288 [label="Expert_Gate.expert_layers.1.layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	139951895940288 -> 139951966735952
	139951966735952 [label=AccumulateGrad]
	139951966735760 -> 139951966735472
	139951895940368 [label="Expert_Gate.expert_layers.1.layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	139951895940368 -> 139951966735760
	139951966735760 [label=AccumulateGrad]
	139951966735712 -> 139951966735472
	139951895940448 [label="Expert_Gate.expert_layers.1.layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	139951895940448 -> 139951966735712
	139951966735712 [label=AccumulateGrad]
	139951966735328 -> 139951966735136
	139951881966112 [label="Expert_Gate.expert_layers.1.layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951881966112 -> 139951966735328
	139951966735328 [label=AccumulateGrad]
	139951966735088 -> 139951966735040
	139951881966192 [label="Expert_Gate.expert_layers.1.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	139951881966192 -> 139951966735088
	139951966735088 [label=AccumulateGrad]
	139951966734944 -> 139951966735040
	139951881966272 [label="Expert_Gate.expert_layers.1.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	139951881966272 -> 139951966734944
	139951966734944 [label=AccumulateGrad]
	139951966734848 -> 139951966734704
	139951881966512 [label="Expert_Gate.expert_layers.1.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951881966512 -> 139951966734848
	139951966734848 [label=AccumulateGrad]
	139951966734656 -> 139951966734608
	139951881966432 [label="Expert_Gate.expert_layers.1.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	139951881966432 -> 139951966734656
	139951966734656 [label=AccumulateGrad]
	139951966734512 -> 139951966734608
	139951881966592 [label="Expert_Gate.expert_layers.1.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	139951881966592 -> 139951966734512
	139951966734512 [label=AccumulateGrad]
	139951966734416 -> 139951966734272
	139951881966752 [label="Expert_Gate.expert_layers.1.layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951881966752 -> 139951966734416
	139951966734416 [label=AccumulateGrad]
	139951966734224 -> 139951966734128
	139951881966832 [label="Expert_Gate.expert_layers.1.layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	139951881966832 -> 139951966734224
	139951966734224 [label=AccumulateGrad]
	139951966734176 -> 139951966734128
	139951881966912 [label="Expert_Gate.expert_layers.1.layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	139951881966912 -> 139951966734176
	139951966734176 [label=AccumulateGrad]
	139951966734080 -> 139951966734032
	139951966733936 -> 139951966733744
	139951881967072 [label="Expert_Gate.expert_layers.1.layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951881967072 -> 139951966733936
	139951966733936 [label=AccumulateGrad]
	139951966733696 -> 139951966733648
	139951881967152 [label="Expert_Gate.expert_layers.1.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	139951881967152 -> 139951966733696
	139951966733696 [label=AccumulateGrad]
	139951966733552 -> 139951966733648
	139951881967232 [label="Expert_Gate.expert_layers.1.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	139951881967232 -> 139951966733552
	139951966733552 [label=AccumulateGrad]
	139951966733456 -> 139951966733312
	139951881967472 [label="Expert_Gate.expert_layers.1.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951881967472 -> 139951966733456
	139951966733456 [label=AccumulateGrad]
	139951966733264 -> 139951966733216
	139951881967392 [label="Expert_Gate.expert_layers.1.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	139951881967392 -> 139951966733264
	139951966733264 [label=AccumulateGrad]
	139951966733120 -> 139951966733216
	139951881967552 [label="Expert_Gate.expert_layers.1.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	139951881967552 -> 139951966733120
	139951966733120 [label=AccumulateGrad]
	139951966733024 -> 139951966732880
	139951881967712 [label="Expert_Gate.expert_layers.1.layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951881967712 -> 139951966733024
	139951966733024 [label=AccumulateGrad]
	139951966732832 -> 139951966732736
	139951881967792 [label="Expert_Gate.expert_layers.1.layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	139951881967792 -> 139951966732832
	139951966732832 [label=AccumulateGrad]
	139951966732784 -> 139951966732736
	139951881967872 [label="Expert_Gate.expert_layers.1.layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	139951881967872 -> 139951966732784
	139951966732784 [label=AccumulateGrad]
	139951966732688 -> 139951966732640
	139951966732544 -> 139951966695376
	139951881968032 [label="Expert_Gate.expert_layers.1.layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951881968032 -> 139951966732544
	139951966732544 [label=AccumulateGrad]
	139951966695232 -> 139951966695328
	139951881968112 [label="Expert_Gate.expert_layers.1.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	139951881968112 -> 139951966695232
	139951966695232 [label=AccumulateGrad]
	139951966732352 -> 139951966695328
	139951881968192 [label="Expert_Gate.expert_layers.1.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	139951881968192 -> 139951966732352
	139951966732352 [label=AccumulateGrad]
	139951966695136 -> 139951966694992
	139951881968432 [label="Expert_Gate.expert_layers.1.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951881968432 -> 139951966695136
	139951966695136 [label=AccumulateGrad]
	139951966694944 -> 139951966694896
	139951881968352 [label="Expert_Gate.expert_layers.1.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	139951881968352 -> 139951966694944
	139951966694944 [label=AccumulateGrad]
	139951966694800 -> 139951966694896
	139951881968512 [label="Expert_Gate.expert_layers.1.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	139951881968512 -> 139951966694800
	139951966694800 [label=AccumulateGrad]
	139951966694704 -> 139951966694560
	139951881968672 [label="Expert_Gate.expert_layers.1.layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951881968672 -> 139951966694704
	139951966694704 [label=AccumulateGrad]
	139951966694512 -> 139951966694416
	139951881968752 [label="Expert_Gate.expert_layers.1.layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	139951881968752 -> 139951966694512
	139951966694512 [label=AccumulateGrad]
	139951966694464 -> 139951966694416
	139951881968832 [label="Expert_Gate.expert_layers.1.layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	139951881968832 -> 139951966694464
	139951966694464 [label=AccumulateGrad]
	139951966694368 -> 139951966694320
	139951966694032 -> 139951966694224
	139951881969072 [label="Expert_Gate.expert_layers.1.layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951881969072 -> 139951966694032
	139951966694032 [label=AccumulateGrad]
	139951966694080 -> 139951966692784
	139951881969152 [label="Expert_Gate.expert_layers.1.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	139951881969152 -> 139951966694080
	139951966694080 [label=AccumulateGrad]
	139951966693552 -> 139951966692784
	139951881969232 [label="Expert_Gate.expert_layers.1.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	139951881969232 -> 139951966693552
	139951966693552 [label=AccumulateGrad]
	139951966692592 -> 139951966692160
	139951881969472 [label="Expert_Gate.expert_layers.1.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951881969472 -> 139951966692592
	139951966692592 [label=AccumulateGrad]
	139951966692208 -> 139951966691776
	139951881969392 [label="Expert_Gate.expert_layers.1.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	139951881969392 -> 139951966692208
	139951966692208 [label=AccumulateGrad]
	139951966691440 -> 139951966691776
	139951881969552 [label="Expert_Gate.expert_layers.1.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	139951881969552 -> 139951966691440
	139951966691440 [label=AccumulateGrad]
	139951966682944 -> 139951966682320
	139951872422032 [label="Expert_Gate.expert_layers.1.layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951872422032 -> 139951966682944
	139951966682944 [label=AccumulateGrad]
	139951966682512 -> 139951966682128
	139951872422112 [label="Expert_Gate.expert_layers.1.layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	139951872422112 -> 139951966682512
	139951966682512 [label=AccumulateGrad]
	139951966682560 -> 139951966682128
	139951872422192 [label="Expert_Gate.expert_layers.1.layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	139951872422192 -> 139951966682560
	139951966682560 [label=AccumulateGrad]
	139951966681744 -> 139951966681600
	139951966681360 -> 139951966681120
	139951872422352 [label="Expert_Gate.expert_layers.1.layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951872422352 -> 139951966681360
	139951966681360 [label=AccumulateGrad]
	139951966681168 -> 139951966680736
	139951872422432 [label="Expert_Gate.expert_layers.1.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	139951872422432 -> 139951966681168
	139951966681168 [label=AccumulateGrad]
	139951966680208 -> 139951966680736
	139951872422512 [label="Expert_Gate.expert_layers.1.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	139951872422512 -> 139951966680208
	139951966680208 [label=AccumulateGrad]
	139951966679968 -> 139951966679536
	139951872422752 [label="Expert_Gate.expert_layers.1.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951872422752 -> 139951966679968
	139951966679968 [label=AccumulateGrad]
	139951966679728 -> 139951966679776
	139951872422672 [label="Expert_Gate.expert_layers.1.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	139951872422672 -> 139951966679728
	139951966679728 [label=AccumulateGrad]
	139951966679104 -> 139951966679776
	139951872422832 [label="Expert_Gate.expert_layers.1.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	139951872422832 -> 139951966679104
	139951966679104 [label=AccumulateGrad]
	139951966645744 -> 139951966646032
	139951872422992 [label="Expert_Gate.expert_layers.1.layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951872422992 -> 139951966645744
	139951966645744 [label=AccumulateGrad]
	139951966645312 -> 139951966645552
	139951872423072 [label="Expert_Gate.expert_layers.1.layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	139951872423072 -> 139951966645312
	139951966645312 [label=AccumulateGrad]
	139951966645504 -> 139951966645552
	139951872423152 [label="Expert_Gate.expert_layers.1.layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	139951872423152 -> 139951966645504
	139951966645504 [label=AccumulateGrad]
	139951966645120 -> 139951966645072
	139951966644496 -> 139951966644448
	139951966644496 [label=ReluBackward0]
	139951966645936 -> 139951966644496
	139951966645936 [label=AddBackward0]
	139951966644688 -> 139951966645936
	139951966644688 [label=NativeGroupNormBackward0]
	139951966681552 -> 139951966644688
	139951966681552 [label=ConvolutionBackward0]
	139951966681648 -> 139951966681552
	139951966681648 [label=ReluBackward0]
	139951966682080 -> 139951966681648
	139951966682080 [label=NativeGroupNormBackward0]
	139951966691392 -> 139951966682080
	139951966691392 [label=ConvolutionBackward0]
	139951966694608 -> 139951966691392
	139951966694608 [label=ReluBackward0]
	139951966694656 -> 139951966694608
	139951966694656 [label=NativeGroupNormBackward0]
	139951966695040 -> 139951966694656
	139951966695040 [label=ConvolutionBackward0]
	139951966680160 -> 139951966695040
	139951966680160 [label=ReluBackward0]
	139951966732448 -> 139951966680160
	139951966732448 [label=AddBackward0]
	139951966732976 -> 139951966732448
	139951966732976 [label=NativeGroupNormBackward0]
	139951966733168 -> 139951966732976
	139951966733168 [label=ConvolutionBackward0]
	139951966734320 -> 139951966733168
	139951966734320 [label=ReluBackward0]
	139951966734368 -> 139951966734320
	139951966734368 [label=NativeGroupNormBackward0]
	139951966734752 -> 139951966734368
	139951966734752 [label=ConvolutionBackward0]
	139951966734992 -> 139951966734752
	139951966734992 [label=ReluBackward0]
	139951966735376 -> 139951966734992
	139951966735376 [label=NativeGroupNormBackward0]
	139951966736144 -> 139951966735376
	139951966736144 [label=ConvolutionBackward0]
	139951966732592 -> 139951966736144
	139951966732592 [label=ReluBackward0]
	139951966741072 -> 139951966732592
	139951966741072 [label=AddBackward0]
	139951966741168 -> 139951966741072
	139951966741168 [label=NativeGroupNormBackward0]
	139951966741312 -> 139951966741168
	139951966741312 [label=ConvolutionBackward0]
	139951966741504 -> 139951966741312
	139951966741504 [label=ReluBackward0]
	139951966741648 -> 139951966741504
	139951966741648 [label=NativeGroupNormBackward0]
	139951966741744 -> 139951966741648
	139951966741744 [label=ConvolutionBackward0]
	139951966741936 -> 139951966741744
	139951966741936 [label=ReluBackward0]
	139951966742080 -> 139951966741936
	139951966742080 [label=NativeGroupNormBackward0]
	139951966742176 -> 139951966742080
	139951966742176 [label=ConvolutionBackward0]
	139951966741120 -> 139951966742176
	139951966741120 [label=ReluBackward0]
	139951966742464 -> 139951966741120
	139951966742464 [label=AddBackward0]
	139951966742560 -> 139951966742464
	139951966742560 [label=NativeGroupNormBackward0]
	139951966742704 -> 139951966742560
	139951966742704 [label=ConvolutionBackward0]
	139951966742896 -> 139951966742704
	139951966742896 [label=ReluBackward0]
	139951966743040 -> 139951966742896
	139951966743040 [label=NativeGroupNormBackward0]
	139951966743136 -> 139951966743040
	139951966743136 [label=ConvolutionBackward0]
	139951966743328 -> 139951966743136
	139951966743328 [label=ReluBackward0]
	139951966743472 -> 139951966743328
	139951966743472 [label=NativeGroupNormBackward0]
	139951966743568 -> 139951966743472
	139951966743568 [label=ConvolutionBackward0]
	139951966742512 -> 139951966743568
	139951966742512 [label=ReluBackward0]
	139951966743856 -> 139951966742512
	139951966743856 [label=AddBackward0]
	139951966743952 -> 139951966743856
	139951966743952 [label=NativeGroupNormBackward0]
	139951966744096 -> 139951966743952
	139951966744096 [label=ConvolutionBackward0]
	139951966744288 -> 139951966744096
	139951966744288 [label=ReluBackward0]
	139951966744432 -> 139951966744288
	139951966744432 [label=NativeGroupNormBackward0]
	139951966744528 -> 139951966744432
	139951966744528 [label=ConvolutionBackward0]
	139951966261456 -> 139951966744528
	139951966261456 [label=ReluBackward0]
	139951966261600 -> 139951966261456
	139951966261600 [label=NativeGroupNormBackward0]
	139951966261696 -> 139951966261600
	139951966261696 [label=ConvolutionBackward0]
	139951966743904 -> 139951966261696
	139951966743904 [label=ReluBackward0]
	139951966261984 -> 139951966743904
	139951966261984 [label=AddBackward0]
	139951966262080 -> 139951966261984
	139951966262080 [label=NativeGroupNormBackward0]
	139951966262224 -> 139951966262080
	139951966262224 [label=ConvolutionBackward0]
	139951966262416 -> 139951966262224
	139951966262416 [label=ReluBackward0]
	139951966262560 -> 139951966262416
	139951966262560 [label=NativeGroupNormBackward0]
	139951966262656 -> 139951966262560
	139951966262656 [label=ConvolutionBackward0]
	139951966262848 -> 139951966262656
	139951966262848 [label=ReluBackward0]
	139951966262992 -> 139951966262848
	139951966262992 [label=NativeGroupNormBackward0]
	139951966263088 -> 139951966262992
	139951966263088 [label=ConvolutionBackward0]
	139951966564992 -> 139951966263088
	139951966263280 -> 139951966263088
	139951805035120 [label="Expert_Gate.expert_layers.2.layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	139951805035120 -> 139951966263280
	139951966263280 [label=AccumulateGrad]
	139951966263040 -> 139951966262992
	139951805035200 [label="Expert_Gate.expert_layers.2.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	139951805035200 -> 139951966263040
	139951966263040 [label=AccumulateGrad]
	139951966262896 -> 139951966262992
	139951805035280 [label="Expert_Gate.expert_layers.2.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	139951805035280 -> 139951966262896
	139951966262896 [label=AccumulateGrad]
	139951966262800 -> 139951966262656
	139951805035520 [label="Expert_Gate.expert_layers.2.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951805035520 -> 139951966262800
	139951966262800 [label=AccumulateGrad]
	139951966262608 -> 139951966262560
	139951805035440 [label="Expert_Gate.expert_layers.2.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	139951805035440 -> 139951966262608
	139951966262608 [label=AccumulateGrad]
	139951966262464 -> 139951966262560
	139951805035600 [label="Expert_Gate.expert_layers.2.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	139951805035600 -> 139951966262464
	139951966262464 [label=AccumulateGrad]
	139951966262368 -> 139951966262224
	139951805035760 [label="Expert_Gate.expert_layers.2.layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951805035760 -> 139951966262368
	139951966262368 [label=AccumulateGrad]
	139951966262176 -> 139951966262080
	139951805035840 [label="Expert_Gate.expert_layers.2.layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	139951805035840 -> 139951966262176
	139951966262176 [label=AccumulateGrad]
	139951966262128 -> 139951966262080
	139951805035920 [label="Expert_Gate.expert_layers.2.layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	139951805035920 -> 139951966262128
	139951966262128 [label=AccumulateGrad]
	139951966262032 -> 139951966261984
	139951966262032 [label=NativeGroupNormBackward0]
	139951966262752 -> 139951966262032
	139951966262752 [label=ConvolutionBackward0]
	139951966564992 -> 139951966262752
	139951966263136 -> 139951966262752
	139951805034800 [label="Expert_Gate.expert_layers.2.layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	139951805034800 -> 139951966263136
	139951966263136 [label=AccumulateGrad]
	139951966262320 -> 139951966262032
	139951805034880 [label="Expert_Gate.expert_layers.2.layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	139951805034880 -> 139951966262320
	139951966262320 [label=AccumulateGrad]
	139951966262272 -> 139951966262032
	139951805034960 [label="Expert_Gate.expert_layers.2.layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	139951805034960 -> 139951966262272
	139951966262272 [label=AccumulateGrad]
	139951966261888 -> 139951966261696
	139951805036080 [label="Expert_Gate.expert_layers.2.layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951805036080 -> 139951966261888
	139951966261888 [label=AccumulateGrad]
	139951966261648 -> 139951966261600
	139951805036160 [label="Expert_Gate.expert_layers.2.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	139951805036160 -> 139951966261648
	139951966261648 [label=AccumulateGrad]
	139951966261504 -> 139951966261600
	139951805036240 [label="Expert_Gate.expert_layers.2.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	139951805036240 -> 139951966261504
	139951966261504 [label=AccumulateGrad]
	139951966261408 -> 139951966744528
	139951805036480 [label="Expert_Gate.expert_layers.2.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951805036480 -> 139951966261408
	139951966261408 [label=AccumulateGrad]
	139951966744480 -> 139951966744432
	139951805036400 [label="Expert_Gate.expert_layers.2.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	139951805036400 -> 139951966744480
	139951966744480 [label=AccumulateGrad]
	139951966744336 -> 139951966744432
	139951805036560 [label="Expert_Gate.expert_layers.2.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	139951805036560 -> 139951966744336
	139951966744336 [label=AccumulateGrad]
	139951966744240 -> 139951966744096
	139951805036720 [label="Expert_Gate.expert_layers.2.layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951805036720 -> 139951966744240
	139951966744240 [label=AccumulateGrad]
	139951966744048 -> 139951966743952
	139951805036800 [label="Expert_Gate.expert_layers.2.layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	139951805036800 -> 139951966744048
	139951966744048 [label=AccumulateGrad]
	139951966744000 -> 139951966743952
	139951805036880 [label="Expert_Gate.expert_layers.2.layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	139951805036880 -> 139951966744000
	139951966744000 [label=AccumulateGrad]
	139951966743904 -> 139951966743856
	139951966743760 -> 139951966743568
	139951805037040 [label="Expert_Gate.expert_layers.2.layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951805037040 -> 139951966743760
	139951966743760 [label=AccumulateGrad]
	139951966743520 -> 139951966743472
	139951805037120 [label="Expert_Gate.expert_layers.2.layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	139951805037120 -> 139951966743520
	139951966743520 [label=AccumulateGrad]
	139951966743376 -> 139951966743472
	139951805037200 [label="Expert_Gate.expert_layers.2.layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	139951805037200 -> 139951966743376
	139951966743376 [label=AccumulateGrad]
	139951966743280 -> 139951966743136
	139951805037440 [label="Expert_Gate.expert_layers.2.layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951805037440 -> 139951966743280
	139951966743280 [label=AccumulateGrad]
	139951966743088 -> 139951966743040
	139951805037360 [label="Expert_Gate.expert_layers.2.layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	139951805037360 -> 139951966743088
	139951966743088 [label=AccumulateGrad]
	139951966742944 -> 139951966743040
	139951805037520 [label="Expert_Gate.expert_layers.2.layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	139951805037520 -> 139951966742944
	139951966742944 [label=AccumulateGrad]
	139951966742848 -> 139951966742704
	139951805037680 [label="Expert_Gate.expert_layers.2.layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951805037680 -> 139951966742848
	139951966742848 [label=AccumulateGrad]
	139951966742656 -> 139951966742560
	139951805037760 [label="Expert_Gate.expert_layers.2.layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	139951805037760 -> 139951966742656
	139951966742656 [label=AccumulateGrad]
	139951966742608 -> 139951966742560
	139951805037840 [label="Expert_Gate.expert_layers.2.layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	139951805037840 -> 139951966742608
	139951966742608 [label=AccumulateGrad]
	139951966742512 -> 139951966742464
	139951966742368 -> 139951966742176
	139951805038000 [label="Expert_Gate.expert_layers.2.layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951805038000 -> 139951966742368
	139951966742368 [label=AccumulateGrad]
	139951966742128 -> 139951966742080
	139951805038080 [label="Expert_Gate.expert_layers.2.layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	139951805038080 -> 139951966742128
	139951966742128 [label=AccumulateGrad]
	139951966741984 -> 139951966742080
	139951805038160 [label="Expert_Gate.expert_layers.2.layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	139951805038160 -> 139951966741984
	139951966741984 [label=AccumulateGrad]
	139951966741888 -> 139951966741744
	139951805038400 [label="Expert_Gate.expert_layers.2.layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951805038400 -> 139951966741888
	139951966741888 [label=AccumulateGrad]
	139951966741696 -> 139951966741648
	139951805038320 [label="Expert_Gate.expert_layers.2.layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	139951805038320 -> 139951966741696
	139951966741696 [label=AccumulateGrad]
	139951966741552 -> 139951966741648
	139951805038480 [label="Expert_Gate.expert_layers.2.layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	139951805038480 -> 139951966741552
	139951966741552 [label=AccumulateGrad]
	139951966741456 -> 139951966741312
	139951797870736 [label="Expert_Gate.expert_layers.2.layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951797870736 -> 139951966741456
	139951966741456 [label=AccumulateGrad]
	139951966741264 -> 139951966741168
	139951797870816 [label="Expert_Gate.expert_layers.2.layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	139951797870816 -> 139951966741264
	139951966741264 [label=AccumulateGrad]
	139951966741216 -> 139951966741168
	139951797870896 [label="Expert_Gate.expert_layers.2.layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	139951797870896 -> 139951966741216
	139951966741216 [label=AccumulateGrad]
	139951966741120 -> 139951966741072
	139951966740784 -> 139951966736144
	139951797871056 [label="Expert_Gate.expert_layers.2.layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951797871056 -> 139951966740784
	139951966740784 [label=AccumulateGrad]
	139951966740976 -> 139951966735376
	139951797871136 [label="Expert_Gate.expert_layers.2.layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	139951797871136 -> 139951966740976
	139951966740976 [label=AccumulateGrad]
	139951966740736 -> 139951966735376
	139951797871216 [label="Expert_Gate.expert_layers.2.layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	139951797871216 -> 139951966740736
	139951966740736 [label=AccumulateGrad]
	139951966735184 -> 139951966734752
	139951797871456 [label="Expert_Gate.expert_layers.2.layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951797871456 -> 139951966735184
	139951966735184 [label=AccumulateGrad]
	139951966734800 -> 139951966734368
	139951797871376 [label="Expert_Gate.expert_layers.2.layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	139951797871376 -> 139951966734800
	139951966734800 [label=AccumulateGrad]
	139951966733840 -> 139951966734368
	139951797871536 [label="Expert_Gate.expert_layers.2.layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	139951797871536 -> 139951966733840
	139951966733840 [label=AccumulateGrad]
	139951966733600 -> 139951966733168
	139951797871696 [label="Expert_Gate.expert_layers.2.layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951797871696 -> 139951966733600
	139951966733600 [label=AccumulateGrad]
	139951966733360 -> 139951966732976
	139951797871776 [label="Expert_Gate.expert_layers.2.layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	139951797871776 -> 139951966733360
	139951966733360 [label=AccumulateGrad]
	139951966733408 -> 139951966732976
	139951797871856 [label="Expert_Gate.expert_layers.2.layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	139951797871856 -> 139951966733408
	139951966733408 [label=AccumulateGrad]
	139951966732592 -> 139951966732448
	139951966695280 -> 139951966695040
	139951797872016 [label="Expert_Gate.expert_layers.2.layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	139951797872016 -> 139951966695280
	139951966695280 [label=AccumulateGrad]
	139951966695088 -> 139951966694656
	139951797872096 [label="Expert_Gate.expert_layers.2.layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	139951797872096 -> 139951966695088
	139951966695088 [label=AccumulateGrad]
	139951966693360 -> 139951966694656
	139951797872176 [label="Expert_Gate.expert_layers.2.layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	139951797872176 -> 139951966693360
	139951966693360 [label=AccumulateGrad]
	139951966692640 -> 139951966691392
	139951797872416 [label="Expert_Gate.expert_layers.2.layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139951797872416 -> 139951966692640
	139951966692640 [label=AccumulateGrad]
	139951966691968 -> 139951966682080
	139951797872336 [label="Expert_Gate.expert_layers.2.layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	139951797872336 -> 139951966691968
	139951966691968 [label=AccumulateGrad]
	139951966691728 -> 139951966682080
	139951797872496 [label="Expert_Gate.expert_layers.2.layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	139951797872496 -> 139951966691728
	139951966691728 [label=AccumulateGrad]
	139951966683040 -> 139951966681552
	139951797872656 [label="Expert_Gate.expert_layers.2.layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	139951797872656 -> 139951966683040
	139951966683040 [label=AccumulateGrad]
	139951966679344 -> 139951966644688
	139951797872736 [label="Expert_Gate.expert_layers.2.layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	139951797872736 -> 139951966679344
	139951966679344 [label=AccumulateGrad]
	139951966680256 -> 139951966644688
	139951797872816 [label="Expert_Gate.expert_layers.2.layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	139951797872816 -> 139951966680256
	139951966680256 [label=AccumulateGrad]
	139951966680160 -> 139951966645936
	139951966644400 -> 139951966644256
	139951966644400 [label=AddBackward0]
	139951966644736 -> 139951966644400
	139951966644736 [label=TanhBackward0]
	139951966680352 -> 139951966644736
	139951966680352 [label=AddBackward0]
	139951966679296 -> 139951966680352
	139951966679296 [label=MulBackward0]
	139951966694272 -> 139951966679296
	139951966694272 [label=MulBackward0]
	139951966732496 -> 139951966694272
	139951966732496 [label=PowBackward0]
	139951966735280 -> 139951966732496
	139951966735280 [label=AddBackward0]
	139951966733984 -> 139951966735280
	139951966733984 [label=SumBackward1]
	139951966735232 -> 139951966733984
	139951966735232 [label=PowBackward0]
	139951966644448 -> 139951966735232
	139951966733888 -> 139951966694272
	139951746237120 [label="Expert_Gate.fusion_modules.3.alpha
 (1, 3072, 1, 1)" fillcolor=lightblue]
	139951746237120 -> 139951966733888
	139951966733888 [label=AccumulateGrad]
	139951966694848 -> 139951966679296
	139951966694848 [label=DivBackward0]
	139951966734560 -> 139951966694848
	139951746237200 [label="Expert_Gate.fusion_modules.3.gamma
 (1, 3072, 1, 1)" fillcolor=lightblue]
	139951746237200 -> 139951966734560
	139951966734560 [label=AccumulateGrad]
	139951966732928 -> 139951966694848
	139951966732928 [label=PowBackward0]
	139951966733792 -> 139951966732928
	139951966733792 [label=AddBackward0]
	139951966740832 -> 139951966733792
	139951966740832 [label=MeanBackward1]
	139951966741408 -> 139951966740832
	139951966741408 [label=PowBackward0]
	139951966694272 -> 139951966741408
	139951966694176 -> 139951966680352
	139951746237280 [label="Expert_Gate.fusion_modules.3.beta
 (1, 3072, 1, 1)" fillcolor=lightblue]
	139951746237280 -> 139951966694176
	139951966694176 [label=AccumulateGrad]
	139951966643440 -> 139951966644160
	139951746237600 [label="Expert_Gate.fusion_modules.3.conv.weight
 (1024, 3072, 1, 1)" fillcolor=lightblue]
	139951746237600 -> 139951966643440
	139951966643440 [label=AccumulateGrad]
	139951966643872 -> 139951966644160
	139951746237680 [label="Expert_Gate.fusion_modules.3.conv.bias
 (1024)" fillcolor=lightblue]
	139951746237680 -> 139951966643872
	139951966643872 [label=AccumulateGrad]
	139951966642864 -> 139951966642720
	139951746237760 [label="Expert_Gate.fusion_modules.3.bn.weight
 (1024)" fillcolor=lightblue]
	139951746237760 -> 139951966642864
	139951966642864 [label=AccumulateGrad]
	139951966642480 -> 139951966642720
	139951746237840 [label="Expert_Gate.fusion_modules.3.bn.bias
 (1024)" fillcolor=lightblue]
	139951746237840 -> 139951966642480
	139951966642480 [label=AccumulateGrad]
	139951966642768 -> 139951966612352
	139951966642768 [label=UpsampleBilinear2DBackward0]
	139951966644112 -> 139951966642768
	139951966644112 [label=ReluBackward0]
	139951966644640 -> 139951966644112
	139951966644640 [label=NativeGroupNormBackward0]
	139951966732400 -> 139951966644640
	139951966732400 [label=ConvolutionBackward0]
	139951966692688 -> 139951966732400
	139951966692688 [label=MulBackward0]
	139951966740592 -> 139951966692688
	139951966740592 [label=CatBackward0]
	139951966742224 -> 139951966740592
	139951966742224 [label=ReluBackward0]
	139951966742416 -> 139951966742224
	139951966742416 [label=AddBackward0]
	139951966743232 -> 139951966742416
	139951966743232 [label=NativeGroupNormBackward0]
	139951966743712 -> 139951966743232
	139951966743712 [label=ConvolutionBackward0]
	139951966743664 -> 139951966743712
	139951966743664 [label=ReluBackward0]
	139951966744384 -> 139951966743664
	139951966744384 [label=NativeGroupNormBackward0]
	139951966743808 -> 139951966744384
	139951966743808 [label=ConvolutionBackward0]
	139951966262704 -> 139951966743808
	139951966262704 [label=ReluBackward0]
	139951966263232 -> 139951966262704
	139951966263232 [label=NativeGroupNormBackward0]
	139951966262944 -> 139951966263232
	139951966262944 [label=ConvolutionBackward0]
	139951966742800 -> 139951966262944
	139951966742800 [label=ReluBackward0]
	139951966263520 -> 139951966742800
	139951966263520 [label=AddBackward0]
	139951966263616 -> 139951966263520
	139951966263616 [label=NativeGroupNormBackward0]
	139951966263760 -> 139951966263616
	139951966263760 [label=ConvolutionBackward0]
	139951966263952 -> 139951966263760
	139951966263952 [label=ReluBackward0]
	139951966264096 -> 139951966263952
	139951966264096 [label=NativeGroupNormBackward0]
	139951966264192 -> 139951966264096
	139951966264192 [label=ConvolutionBackward0]
	139951966264384 -> 139951966264192
	139951966264384 [label=ReluBackward0]
	139951966264528 -> 139951966264384
	139951966264528 [label=NativeGroupNormBackward0]
	139951966264624 -> 139951966264528
	139951966264624 [label=ConvolutionBackward0]
	139951966263568 -> 139951966264624
	139951966263568 [label=ReluBackward0]
	139951966264912 -> 139951966263568
	139951966264912 [label=AddBackward0]
	139951966265008 -> 139951966264912
	139951966265008 [label=NativeGroupNormBackward0]
	139951966265152 -> 139951966265008
	139951966265152 [label=ConvolutionBackward0]
	139951966265296 -> 139951966265152
	139951966265296 [label=ReluBackward0]
	139951966302416 -> 139951966265296
	139951966302416 [label=NativeGroupNormBackward0]
	139951966302512 -> 139951966302416
	139951966302512 [label=ConvolutionBackward0]
	139951966302704 -> 139951966302512
	139951966302704 [label=ReluBackward0]
	139951966302848 -> 139951966302704
	139951966302848 [label=NativeGroupNormBackward0]
	139951966302944 -> 139951966302848
	139951966302944 [label=ConvolutionBackward0]
	139951966644592 -> 139951966302944
	139951966303136 -> 139951966302944
	139951949329088 [label="Expert_Gate.expert_layers.0.layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	139951949329088 -> 139951966303136
	139951966303136 [label=AccumulateGrad]
	139951966302896 -> 139951966302848
	139951949329168 [label="Expert_Gate.expert_layers.0.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	139951949329168 -> 139951966302896
	139951966302896 [label=AccumulateGrad]
	139951966302752 -> 139951966302848
	139951949329248 [label="Expert_Gate.expert_layers.0.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	139951949329248 -> 139951966302752
	139951966302752 [label=AccumulateGrad]
	139951966302656 -> 139951966302512
	139951949329488 [label="Expert_Gate.expert_layers.0.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139951949329488 -> 139951966302656
	139951966302656 [label=AccumulateGrad]
	139951966302464 -> 139951966302416
	139951949329408 [label="Expert_Gate.expert_layers.0.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	139951949329408 -> 139951966302464
	139951966302464 [label=AccumulateGrad]
	139951966302320 -> 139951966302416
	139951949329568 [label="Expert_Gate.expert_layers.0.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	139951949329568 -> 139951966302320
	139951966302320 [label=AccumulateGrad]
	139951966265248 -> 139951966265152
	139951949329728 [label="Expert_Gate.expert_layers.0.layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139951949329728 -> 139951966265248
	139951966265248 [label=AccumulateGrad]
	139951966265104 -> 139951966265008
	139951949329808 [label="Expert_Gate.expert_layers.0.layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	139951949329808 -> 139951966265104
	139951966265104 [label=AccumulateGrad]
	139951966265056 -> 139951966265008
	139951949329888 [label="Expert_Gate.expert_layers.0.layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	139951949329888 -> 139951966265056
	139951966265056 [label=AccumulateGrad]
	139951966264960 -> 139951966264912
	139951966264960 [label=NativeGroupNormBackward0]
	139951966265200 -> 139951966264960
	139951966265200 [label=ConvolutionBackward0]
	139951966644592 -> 139951966265200
	139951966302992 -> 139951966265200
	139951949328768 [label="Expert_Gate.expert_layers.0.layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	139951949328768 -> 139951966302992
	139951966302992 [label=AccumulateGrad]
	139951966302608 -> 139951966264960
	139951949328848 [label="Expert_Gate.expert_layers.0.layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	139951949328848 -> 139951966302608
	139951966302608 [label=AccumulateGrad]
	139951966302272 -> 139951966264960
	139951949328928 [label="Expert_Gate.expert_layers.0.layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	139951949328928 -> 139951966302272
	139951966302272 [label=AccumulateGrad]
	139951966264816 -> 139951966264624
	139951949330048 [label="Expert_Gate.expert_layers.0.layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	139951949330048 -> 139951966264816
	139951966264816 [label=AccumulateGrad]
	139951966264576 -> 139951966264528
	139951949330128 [label="Expert_Gate.expert_layers.0.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	139951949330128 -> 139951966264576
	139951966264576 [label=AccumulateGrad]
	139951966264432 -> 139951966264528
	139951949330208 [label="Expert_Gate.expert_layers.0.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	139951949330208 -> 139951966264432
	139951966264432 [label=AccumulateGrad]
	139951966264336 -> 139951966264192
	139951949330448 [label="Expert_Gate.expert_layers.0.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139951949330448 -> 139951966264336
	139951966264336 [label=AccumulateGrad]
	139951966264144 -> 139951966264096
	139951949330368 [label="Expert_Gate.expert_layers.0.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	139951949330368 -> 139951966264144
	139951966264144 [label=AccumulateGrad]
	139951966264000 -> 139951966264096
	139951949330528 [label="Expert_Gate.expert_layers.0.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	139951949330528 -> 139951966264000
	139951966264000 [label=AccumulateGrad]
	139951966263904 -> 139951966263760
	139951949330688 [label="Expert_Gate.expert_layers.0.layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139951949330688 -> 139951966263904
	139951966263904 [label=AccumulateGrad]
	139951966263712 -> 139951966263616
	139951949330768 [label="Expert_Gate.expert_layers.0.layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	139951949330768 -> 139951966263712
	139951966263712 [label=AccumulateGrad]
	139951966263664 -> 139951966263616
	139951949330848 [label="Expert_Gate.expert_layers.0.layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	139951949330848 -> 139951966263664
	139951966263664 [label=AccumulateGrad]
	139951966263568 -> 139951966263520
	139951966263424 -> 139951966262944
	139951949331008 [label="Expert_Gate.expert_layers.0.layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	139951949331008 -> 139951966263424
	139951966263424 [label=AccumulateGrad]
	139951966263376 -> 139951966263232
	139951949331088 [label="Expert_Gate.expert_layers.0.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	139951949331088 -> 139951966263376
	139951966263376 [label=AccumulateGrad]
	139951966261792 -> 139951966263232
	139951949331168 [label="Expert_Gate.expert_layers.0.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	139951949331168 -> 139951966261792
	139951966261792 [label=AccumulateGrad]
	139951966261552 -> 139951966743808
	139951949331408 [label="Expert_Gate.expert_layers.0.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139951949331408 -> 139951966261552
	139951966261552 [label=AccumulateGrad]
	139951966261312 -> 139951966744384
	139951949331328 [label="Expert_Gate.expert_layers.0.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	139951949331328 -> 139951966261312
	139951966261312 [label=AccumulateGrad]
	139951966261360 -> 139951966744384
	139951949331488 [label="Expert_Gate.expert_layers.0.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	139951949331488 -> 139951966261360
	139951966261360 [label=AccumulateGrad]
	139951966744144 -> 139951966743712
	139951949331648 [label="Expert_Gate.expert_layers.0.layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139951949331648 -> 139951966744144
	139951966744144 [label=AccumulateGrad]
	139951966742992 -> 139951966743232
	139951949331728 [label="Expert_Gate.expert_layers.0.layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	139951949331728 -> 139951966742992
	139951966742992 [label=AccumulateGrad]
	139951966743184 -> 139951966743232
	139951949331808 [label="Expert_Gate.expert_layers.0.layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	139951949331808 -> 139951966743184
	139951966743184 [label=AccumulateGrad]
	139951966742800 -> 139951966742416
	139951966742320 -> 139951966740592
	139951966742320 [label=ReluBackward0]
	139951966743616 -> 139951966742320
	139951966743616 [label=AddBackward0]
	139951966744192 -> 139951966743616
	139951966744192 [label=NativeGroupNormBackward0]
	139951966263184 -> 139951966744192
	139951966263184 [label=ConvolutionBackward0]
	139951966263328 -> 139951966263184
	139951966263328 [label=ReluBackward0]
	139951966264288 -> 139951966263328
	139951966264288 [label=NativeGroupNormBackward0]
	139951966264048 -> 139951966264288
	139951966264048 [label=ConvolutionBackward0]
	139951966264720 -> 139951966264048
	139951966264720 [label=ReluBackward0]
	139951966264864 -> 139951966264720
	139951966264864 [label=NativeGroupNormBackward0]
	139951966302800 -> 139951966264864
	139951966302800 [label=ConvolutionBackward0]
	139951966743424 -> 139951966302800
	139951966743424 [label=ReluBackward0]
	139951966303376 -> 139951966743424
	139951966303376 [label=AddBackward0]
	139951966303472 -> 139951966303376
	139951966303472 [label=NativeGroupNormBackward0]
	139951966303616 -> 139951966303472
	139951966303616 [label=ConvolutionBackward0]
	139951966303808 -> 139951966303616
	139951966303808 [label=ReluBackward0]
	139951966303952 -> 139951966303808
	139951966303952 [label=NativeGroupNormBackward0]
	139951966304048 -> 139951966303952
	139951966304048 [label=ConvolutionBackward0]
	139951966304240 -> 139951966304048
	139951966304240 [label=ReluBackward0]
	139951966304384 -> 139951966304240
	139951966304384 [label=NativeGroupNormBackward0]
	139951966304480 -> 139951966304384
	139951966304480 [label=ConvolutionBackward0]
	139951966303424 -> 139951966304480
	139951966303424 [label=ReluBackward0]
	139951966304768 -> 139951966303424
	139951966304768 [label=AddBackward0]
	139951966304864 -> 139951966304768
	139951966304864 [label=NativeGroupNormBackward0]
	139951966305008 -> 139951966304864
	139951966305008 [label=ConvolutionBackward0]
	139951966305200 -> 139951966305008
	139951966305200 [label=ReluBackward0]
	139951966305344 -> 139951966305200
	139951966305344 [label=NativeGroupNormBackward0]
	139951966305440 -> 139951966305344
	139951966305440 [label=ConvolutionBackward0]
	139951966305632 -> 139951966305440
	139951966305632 [label=ReluBackward0]
	139951966305776 -> 139951966305632
	139951966305776 [label=NativeGroupNormBackward0]
	139951966305872 -> 139951966305776
	139951966305872 [label=ConvolutionBackward0]
	139951966644544 -> 139951966305872
	139951966306064 -> 139951966305872
	139951872423632 [label="Expert_Gate.expert_layers.1.layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	139951872423632 -> 139951966306064
	139951966306064 [label=AccumulateGrad]
	139951966305824 -> 139951966305776
	139951872423712 [label="Expert_Gate.expert_layers.1.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	139951872423712 -> 139951966305824
	139951966305824 [label=AccumulateGrad]
	139951966305680 -> 139951966305776
	139951872423792 [label="Expert_Gate.expert_layers.1.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	139951872423792 -> 139951966305680
	139951966305680 [label=AccumulateGrad]
	139951966305584 -> 139951966305440
	139951872424032 [label="Expert_Gate.expert_layers.1.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139951872424032 -> 139951966305584
	139951966305584 [label=AccumulateGrad]
	139951966305392 -> 139951966305344
	139951872423952 [label="Expert_Gate.expert_layers.1.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	139951872423952 -> 139951966305392
	139951966305392 [label=AccumulateGrad]
	139951966305248 -> 139951966305344
	139951872424112 [label="Expert_Gate.expert_layers.1.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	139951872424112 -> 139951966305248
	139951966305248 [label=AccumulateGrad]
	139951966305152 -> 139951966305008
	139951872424272 [label="Expert_Gate.expert_layers.1.layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139951872424272 -> 139951966305152
	139951966305152 [label=AccumulateGrad]
	139951966304960 -> 139951966304864
	139951872424352 [label="Expert_Gate.expert_layers.1.layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	139951872424352 -> 139951966304960
	139951966304960 [label=AccumulateGrad]
	139951966304912 -> 139951966304864
	139951872424432 [label="Expert_Gate.expert_layers.1.layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	139951872424432 -> 139951966304912
	139951966304912 [label=AccumulateGrad]
	139951966304816 -> 139951966304768
	139951966304816 [label=NativeGroupNormBackward0]
	139951966305536 -> 139951966304816
	139951966305536 [label=ConvolutionBackward0]
	139951966644544 -> 139951966305536
	139951966305920 -> 139951966305536
	139951872423312 [label="Expert_Gate.expert_layers.1.layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	139951872423312 -> 139951966305920
	139951966305920 [label=AccumulateGrad]
	139951966305104 -> 139951966304816
	139951872423392 [label="Expert_Gate.expert_layers.1.layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	139951872423392 -> 139951966305104
	139951966305104 [label=AccumulateGrad]
	139951966305056 -> 139951966304816
	139951872423472 [label="Expert_Gate.expert_layers.1.layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	139951872423472 -> 139951966305056
	139951966305056 [label=AccumulateGrad]
	139951966304672 -> 139951966304480
	139951872424592 [label="Expert_Gate.expert_layers.1.layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	139951872424592 -> 139951966304672
	139951966304672 [label=AccumulateGrad]
	139951966304432 -> 139951966304384
	139951872424672 [label="Expert_Gate.expert_layers.1.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	139951872424672 -> 139951966304432
	139951966304432 [label=AccumulateGrad]
	139951966304288 -> 139951966304384
	139951872424752 [label="Expert_Gate.expert_layers.1.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	139951872424752 -> 139951966304288
	139951966304288 [label=AccumulateGrad]
	139951966304192 -> 139951966304048
	139951872424992 [label="Expert_Gate.expert_layers.1.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139951872424992 -> 139951966304192
	139951966304192 [label=AccumulateGrad]
	139951966304000 -> 139951966303952
	139951872424912 [label="Expert_Gate.expert_layers.1.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	139951872424912 -> 139951966304000
	139951966304000 [label=AccumulateGrad]
	139951966303856 -> 139951966303952
	139951872425072 [label="Expert_Gate.expert_layers.1.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	139951872425072 -> 139951966303856
	139951966303856 [label=AccumulateGrad]
	139951966303760 -> 139951966303616
	139951872425232 [label="Expert_Gate.expert_layers.1.layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139951872425232 -> 139951966303760
	139951966303760 [label=AccumulateGrad]
	139951966303568 -> 139951966303472
	139951872425312 [label="Expert_Gate.expert_layers.1.layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	139951872425312 -> 139951966303568
	139951966303568 [label=AccumulateGrad]
	139951966303520 -> 139951966303472
	139951872425392 [label="Expert_Gate.expert_layers.1.layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	139951872425392 -> 139951966303520
	139951966303520 [label=AccumulateGrad]
	139951966303424 -> 139951966303376
	139951966303280 -> 139951966302800
	139951872425552 [label="Expert_Gate.expert_layers.1.layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	139951872425552 -> 139951966303280
	139951966303280 [label=AccumulateGrad]
	139951966303232 -> 139951966264864
	139951872425632 [label="Expert_Gate.expert_layers.1.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	139951872425632 -> 139951966303232
	139951966303232 [label=AccumulateGrad]
	139951966302560 -> 139951966264864
	139951872425712 [label="Expert_Gate.expert_layers.1.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	139951872425712 -> 139951966302560
	139951966302560 [label=AccumulateGrad]
	139951966264480 -> 139951966264048
	139951823016000 [label="Expert_Gate.expert_layers.1.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139951823016000 -> 139951966264480
	139951966264480 [label=AccumulateGrad]
	139951966264240 -> 139951966264288
	139951872425872 [label="Expert_Gate.expert_layers.1.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	139951872425872 -> 139951966264240
	139951966264240 [label=AccumulateGrad]
	139951966263472 -> 139951966264288
	139951823016080 [label="Expert_Gate.expert_layers.1.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	139951823016080 -> 139951966263472
	139951966263472 [label=AccumulateGrad]
	139951966263808 -> 139951966263184
	139951823016240 [label="Expert_Gate.expert_layers.1.layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139951823016240 -> 139951966263808
	139951966263808 [label=AccumulateGrad]
	139951966261840 -> 139951966744192
	139951823016320 [label="Expert_Gate.expert_layers.1.layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	139951823016320 -> 139951966261840
	139951966261840 [label=AccumulateGrad]
	139951966261744 -> 139951966744192
	139951823016400 [label="Expert_Gate.expert_layers.1.layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	139951823016400 -> 139951966261744
	139951966261744 [label=AccumulateGrad]
	139951966743424 -> 139951966743616
	139951966741600 -> 139951966740592
	139951966741600 [label=ReluBackward0]
	139951966742272 -> 139951966741600
	139951966742272 [label=AddBackward0]
	139951966264672 -> 139951966742272
	139951966264672 [label=NativeGroupNormBackward0]
	139951966263856 -> 139951966264672
	139951966263856 [label=ConvolutionBackward0]
	139951966303184 -> 139951966263856
	139951966303184 [label=ReluBackward0]
	139951966304144 -> 139951966303184
	139951966304144 [label=NativeGroupNormBackward0]
	139951966303904 -> 139951966304144
	139951966303904 [label=ConvolutionBackward0]
	139951966305488 -> 139951966303904
	139951966305488 [label=ReluBackward0]
	139951966306016 -> 139951966305488
	139951966306016 [label=NativeGroupNormBackward0]
	139951966305728 -> 139951966306016
	139951966305728 [label=ConvolutionBackward0]
	139951966261936 -> 139951966305728
	139951966261936 [label=ReluBackward0]
	139951966306256 -> 139951966261936
	139951966306256 [label=AddBackward0]
	139951966347424 -> 139951966306256
	139951966347424 [label=NativeGroupNormBackward0]
	139951966347568 -> 139951966347424
	139951966347568 [label=ConvolutionBackward0]
	139951966347760 -> 139951966347568
	139951966347760 [label=ReluBackward0]
	139951966347904 -> 139951966347760
	139951966347904 [label=NativeGroupNormBackward0]
	139951966348000 -> 139951966347904
	139951966348000 [label=ConvolutionBackward0]
	139951966348192 -> 139951966348000
	139951966348192 [label=ReluBackward0]
	139951966348336 -> 139951966348192
	139951966348336 [label=NativeGroupNormBackward0]
	139951966348432 -> 139951966348336
	139951966348432 [label=ConvolutionBackward0]
	139951966347376 -> 139951966348432
	139951966347376 [label=ReluBackward0]
	139951966348720 -> 139951966347376
	139951966348720 [label=AddBackward0]
	139951966348816 -> 139951966348720
	139951966348816 [label=NativeGroupNormBackward0]
	139951966348960 -> 139951966348816
	139951966348960 [label=ConvolutionBackward0]
	139951966349152 -> 139951966348960
	139951966349152 [label=ReluBackward0]
	139951966349296 -> 139951966349152
	139951966349296 [label=NativeGroupNormBackward0]
	139951966349392 -> 139951966349296
	139951966349392 [label=ConvolutionBackward0]
	139951966349584 -> 139951966349392
	139951966349584 [label=ReluBackward0]
	139951966349728 -> 139951966349584
	139951966349728 [label=NativeGroupNormBackward0]
	139951966349824 -> 139951966349728
	139951966349824 [label=ConvolutionBackward0]
	139951966644496 -> 139951966349824
	139951966350016 -> 139951966349824
	139951797873296 [label="Expert_Gate.expert_layers.2.layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	139951797873296 -> 139951966350016
	139951966350016 [label=AccumulateGrad]
	139951966349776 -> 139951966349728
	139951797873376 [label="Expert_Gate.expert_layers.2.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	139951797873376 -> 139951966349776
	139951966349776 [label=AccumulateGrad]
	139951966349632 -> 139951966349728
	139951797873456 [label="Expert_Gate.expert_layers.2.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	139951797873456 -> 139951966349632
	139951966349632 [label=AccumulateGrad]
	139951966349536 -> 139951966349392
	139951797873696 [label="Expert_Gate.expert_layers.2.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139951797873696 -> 139951966349536
	139951966349536 [label=AccumulateGrad]
	139951966349344 -> 139951966349296
	139951797873616 [label="Expert_Gate.expert_layers.2.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	139951797873616 -> 139951966349344
	139951966349344 [label=AccumulateGrad]
	139951966349200 -> 139951966349296
	139951797873776 [label="Expert_Gate.expert_layers.2.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	139951797873776 -> 139951966349200
	139951966349200 [label=AccumulateGrad]
	139951966349104 -> 139951966348960
	139951797873936 [label="Expert_Gate.expert_layers.2.layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139951797873936 -> 139951966349104
	139951966349104 [label=AccumulateGrad]
	139951966348912 -> 139951966348816
	139951797874016 [label="Expert_Gate.expert_layers.2.layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	139951797874016 -> 139951966348912
	139951966348912 [label=AccumulateGrad]
	139951966348864 -> 139951966348816
	139951797874096 [label="Expert_Gate.expert_layers.2.layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	139951797874096 -> 139951966348864
	139951966348864 [label=AccumulateGrad]
	139951966348768 -> 139951966348720
	139951966348768 [label=NativeGroupNormBackward0]
	139951966349488 -> 139951966348768
	139951966349488 [label=ConvolutionBackward0]
	139951966644496 -> 139951966349488
	139951966349872 -> 139951966349488
	139951797872976 [label="Expert_Gate.expert_layers.2.layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	139951797872976 -> 139951966349872
	139951966349872 [label=AccumulateGrad]
	139951966349056 -> 139951966348768
	139951797873056 [label="Expert_Gate.expert_layers.2.layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	139951797873056 -> 139951966349056
	139951966349056 [label=AccumulateGrad]
	139951966349008 -> 139951966348768
	139951797873136 [label="Expert_Gate.expert_layers.2.layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	139951797873136 -> 139951966349008
	139951966349008 [label=AccumulateGrad]
	139951966348624 -> 139951966348432
	139951797874256 [label="Expert_Gate.expert_layers.2.layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	139951797874256 -> 139951966348624
	139951966348624 [label=AccumulateGrad]
	139951966348384 -> 139951966348336
	139951797874336 [label="Expert_Gate.expert_layers.2.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	139951797874336 -> 139951966348384
	139951966348384 [label=AccumulateGrad]
	139951966348240 -> 139951966348336
	139951797874416 [label="Expert_Gate.expert_layers.2.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	139951797874416 -> 139951966348240
	139951966348240 [label=AccumulateGrad]
	139951966348144 -> 139951966348000
	139951798050880 [label="Expert_Gate.expert_layers.2.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139951798050880 -> 139951966348144
	139951966348144 [label=AccumulateGrad]
	139951966347952 -> 139951966347904
	139951797874576 [label="Expert_Gate.expert_layers.2.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	139951797874576 -> 139951966347952
	139951966347952 [label=AccumulateGrad]
	139951966347808 -> 139951966347904
	139951798050960 [label="Expert_Gate.expert_layers.2.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	139951798050960 -> 139951966347808
	139951966347808 [label=AccumulateGrad]
	139951966347712 -> 139951966347568
	139951798051120 [label="Expert_Gate.expert_layers.2.layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139951798051120 -> 139951966347712
	139951966347712 [label=AccumulateGrad]
	139951966347520 -> 139951966347424
	139951798051200 [label="Expert_Gate.expert_layers.2.layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	139951798051200 -> 139951966347520
	139951966347520 [label=AccumulateGrad]
	139951966347472 -> 139951966347424
	139951798051280 [label="Expert_Gate.expert_layers.2.layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	139951798051280 -> 139951966347472
	139951966347472 [label=AccumulateGrad]
	139951966347376 -> 139951966306256
	139951966306208 -> 139951966305728
	139951798051440 [label="Expert_Gate.expert_layers.2.layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	139951798051440 -> 139951966306208
	139951966306208 [label=AccumulateGrad]
	139951966306160 -> 139951966306016
	139951798051520 [label="Expert_Gate.expert_layers.2.layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	139951798051520 -> 139951966306160
	139951966306160 [label=AccumulateGrad]
	139951966304576 -> 139951966306016
	139951798051600 [label="Expert_Gate.expert_layers.2.layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	139951798051600 -> 139951966304576
	139951966304576 [label=AccumulateGrad]
	139951966304336 -> 139951966303904
	139951798051840 [label="Expert_Gate.expert_layers.2.layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139951798051840 -> 139951966304336
	139951966304336 [label=AccumulateGrad]
	139951966304096 -> 139951966304144
	139951798051760 [label="Expert_Gate.expert_layers.2.layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	139951798051760 -> 139951966304096
	139951966304096 [label=AccumulateGrad]
	139951966303328 -> 139951966304144
	139951798051920 [label="Expert_Gate.expert_layers.2.layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	139951798051920 -> 139951966303328
	139951966303328 [label=AccumulateGrad]
	139951966303664 -> 139951966263856
	139951798052080 [label="Expert_Gate.expert_layers.2.layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	139951798052080 -> 139951966303664
	139951966303664 [label=AccumulateGrad]
	139951966264768 -> 139951966264672
	139951798052160 [label="Expert_Gate.expert_layers.2.layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	139951798052160 -> 139951966264768
	139951966264768 [label=AccumulateGrad]
	139951966303040 -> 139951966264672
	139951798052240 [label="Expert_Gate.expert_layers.2.layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	139951798052240 -> 139951966303040
	139951966303040 [label=AccumulateGrad]
	139951966261936 -> 139951966742272
	139951966741840 -> 139951966692688
	139951966741840 [label=AddBackward0]
	139951966262512 -> 139951966741840
	139951966262512 [label=TanhBackward0]
	139951966742032 -> 139951966262512
	139951966742032 [label=AddBackward0]
	139951966304624 -> 139951966742032
	139951966304624 [label=MulBackward0]
	139951966305296 -> 139951966304624
	139951966305296 [label=MulBackward0]
	139951966306112 -> 139951966305296
	139951966306112 [label=PowBackward0]
	139951966348096 -> 139951966306112
	139951966348096 [label=AddBackward0]
	139951966347856 -> 139951966348096
	139951966347856 [label=SumBackward1]
	139951966348480 -> 139951966347856
	139951966348480 [label=PowBackward0]
	139951966740592 -> 139951966348480
	139951966304720 -> 139951966305296
	139951746237920 [label="Expert_Gate.fusion_modules.4.alpha
 (1, 6144, 1, 1)" fillcolor=lightblue]
	139951746237920 -> 139951966304720
	139951966304720 [label=AccumulateGrad]
	139951966305968 -> 139951966304624
	139951966305968 [label=DivBackward0]
	139951966348048 -> 139951966305968
	139951746238000 [label="Expert_Gate.fusion_modules.4.gamma
 (1, 6144, 1, 1)" fillcolor=lightblue]
	139951746238000 -> 139951966348048
	139951966348048 [label=AccumulateGrad]
	139951966348576 -> 139951966305968
	139951966348576 [label=PowBackward0]
	139951966347664 -> 139951966348576
	139951966347664 [label=AddBackward0]
	139951966348528 -> 139951966347664
	139951966348528 [label=MeanBackward1]
	139951966349968 -> 139951966348528
	139951966349968 [label=PowBackward0]
	139951966305296 -> 139951966349968
	139951966304528 -> 139951966742032
	139951746238080 [label="Expert_Gate.fusion_modules.4.beta
 (1, 6144, 1, 1)" fillcolor=lightblue]
	139951746238080 -> 139951966304528
	139951966304528 [label=AccumulateGrad]
	139951966740928 -> 139951966732400
	139951746238400 [label="Expert_Gate.fusion_modules.4.conv.weight
 (2048, 6144, 1, 1)" fillcolor=lightblue]
	139951746238400 -> 139951966740928
	139951966740928 [label=AccumulateGrad]
	139951966741360 -> 139951966732400
	139951746238480 [label="Expert_Gate.fusion_modules.4.conv.bias
 (2048)" fillcolor=lightblue]
	139951746238480 -> 139951966741360
	139951966741360 [label=AccumulateGrad]
	139951966680928 -> 139951966644640
	139951746238560 [label="Expert_Gate.fusion_modules.4.bn.weight
 (2048)" fillcolor=lightblue]
	139951746238560 -> 139951966680928
	139951966680928 [label=AccumulateGrad]
	139951966643632 -> 139951966644640
	139951746238640 [label="Expert_Gate.fusion_modules.4.bn.bias
 (2048)" fillcolor=lightblue]
	139951746238640 -> 139951966643632
	139951966643632 [label=AccumulateGrad]
	139951966611056 -> 139951966613312
	139952177073904 [label="up_concat4.0.conv1.weight
 (512, 3072, 3, 3)" fillcolor=lightblue]
	139952177073904 -> 139951966611056
	139951966611056 [label=AccumulateGrad]
	139951966609472 -> 139951966573472
	139952177073984 [label="up_concat4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139952177073984 -> 139951966609472
	139951966609472 [label=AccumulateGrad]
	139951966564848 -> 139951966564800
	139951966564848 [label=ViewBackward0]
	139951966612736 -> 139951966564848
	139951966612736 [label=PermuteBackward0]
	139951966572992 -> 139951966612736
	139951966572992 [label=ViewBackward0]
	139951966642240 -> 139951966572992
	139951966642240 [label=BmmBackward0]
	139951966693792 -> 139951966642240
	139951966693792 [label=UnsafeViewBackward0]
	139951966742752 -> 139951966693792
	139951966742752 [label=CloneBackward0]
	139951966303712 -> 139951966742752
	139951966303712 [label=PermuteBackward0]
	139951966303088 -> 139951966303712
	139951966303088 [label=PermuteBackward0]
	139951966348672 -> 139951966303088
	139951966348672 [label=UnsqueezeBackward0]
	139951966350112 -> 139951966348672
	139951966350112 [label=ReluBackward0]
	139951966349248 -> 139951966350112
	139951966349248 [label=ConvolutionBackward0]
	139951966349920 -> 139951966349248
	139951966349920 [label=ReluBackward0]
	139951966350256 -> 139951966349920
	139951966350256 [label=ConvolutionBackward0]
	139951966350352 -> 139951966350256
	139951966350352 [label=CatBackward0]
	139951966350496 -> 139951966350352
	139951966350496 [label=ReluBackward0]
	139951966350640 -> 139951966350496
	139951966350640 [label=NativeGroupNormBackward0]
	139951966350736 -> 139951966350640
	139951966350736 [label=ConvolutionBackward0]
	139951966350832 -> 139951966350736
	139951966350832 [label=MulBackward0]
	139951966350928 -> 139951966350832
	139951966350928 [label=CatBackward0]
	139951966644592 -> 139951966350928
	139951966644544 -> 139951966350928
	139951966644496 -> 139951966350928
	139951966350880 -> 139951966350832
	139951966350880 [label=AddBackward0]
	139951966350976 -> 139951966350880
	139951966350976 [label=TanhBackward0]
	139951966351168 -> 139951966350976
	139951966351168 [label=AddBackward0]
	139951966351264 -> 139951966351168
	139951966351264 [label=MulBackward0]
	139951966351312 -> 139951966351264
	139951966351312 [label=MulBackward0]
	139951966384336 -> 139951966351312
	139951966384336 [label=PowBackward0]
	139951966384432 -> 139951966384336
	139951966384432 [label=AddBackward0]
	139951966384528 -> 139951966384432
	139951966384528 [label=SumBackward1]
	139951966384624 -> 139951966384528
	139951966384624 [label=PowBackward0]
	139951966350928 -> 139951966384624
	139951966733888 -> 139951966351312
	139951966351072 -> 139951966351264
	139951966351072 [label=DivBackward0]
	139951966734560 -> 139951966351072
	139951966384480 -> 139951966351072
	139951966384480 [label=PowBackward0]
	139951966384720 -> 139951966384480
	139951966384720 [label=AddBackward0]
	139951966384240 -> 139951966384720
	139951966384240 [label=MeanBackward1]
	139951966384816 -> 139951966384240
	139951966384816 [label=PowBackward0]
	139951966351312 -> 139951966384816
	139951966694176 -> 139951966351168
	139951966643440 -> 139951966350736
	139951966643872 -> 139951966350736
	139951966642864 -> 139951966350640
	139951966642480 -> 139951966350640
	139951966350448 -> 139951966350352
	139951966350448 [label=UpsampleBilinear2DBackward0]
	139951966350784 -> 139951966350448
	139951966350784 [label=ReluBackward0]
	139951966351024 -> 139951966350784
	139951966351024 [label=NativeGroupNormBackward0]
	139951966351216 -> 139951966351024
	139951966351216 [label=ConvolutionBackward0]
	139951966350592 -> 139951966351216
	139951966350592 [label=MulBackward0]
	139951966384672 -> 139951966350592
	139951966384672 [label=CatBackward0]
	139951966742224 -> 139951966384672
	139951966742320 -> 139951966384672
	139951966741600 -> 139951966384672
	139951966384576 -> 139951966350592
	139951966384576 [label=AddBackward0]
	139951966384768 -> 139951966384576
	139951966384768 [label=TanhBackward0]
	139951966384960 -> 139951966384768
	139951966384960 [label=AddBackward0]
	139951966385056 -> 139951966384960
	139951966385056 [label=MulBackward0]
	139951966385152 -> 139951966385056
	139951966385152 [label=MulBackward0]
	139951966385296 -> 139951966385152
	139951966385296 [label=PowBackward0]
	139951966385392 -> 139951966385296
	139951966385392 [label=AddBackward0]
	139951966385488 -> 139951966385392
	139951966385488 [label=SumBackward1]
	139951966385584 -> 139951966385488
	139951966385584 [label=PowBackward0]
	139951966384672 -> 139951966385584
	139951966304720 -> 139951966385152
	139951966385104 -> 139951966385056
	139951966385104 [label=DivBackward0]
	139951966348048 -> 139951966385104
	139951966385440 -> 139951966385104
	139951966385440 [label=PowBackward0]
	139951966385680 -> 139951966385440
	139951966385680 [label=AddBackward0]
	139951966385200 -> 139951966385680
	139951966385200 [label=MeanBackward1]
	139951966385776 -> 139951966385200
	139951966385776 [label=PowBackward0]
	139951966385152 -> 139951966385776
	139951966304528 -> 139951966384960
	139951966740928 -> 139951966351216
	139951966741360 -> 139951966351216
	139951966680928 -> 139951966351024
	139951966643632 -> 139951966351024
	139951966350304 -> 139951966350256
	139952177073744 [label="up_concat4.1.conv1.weight
 (512, 3072, 3, 3)" fillcolor=lightblue]
	139952177073744 -> 139951966350304
	139951966350304 [label=AccumulateGrad]
	139951966350064 -> 139951966349248
	139952177073264 [label="up_concat4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139952177073264 -> 139951966350064
	139951966350064 [label=AccumulateGrad]
	139951966644352 -> 139951966642240
	139951966644352 [label=ViewBackward0]
	139951966302368 -> 139951966644352
	139951966302368 [label=PermuteBackward0]
	139951966349680 -> 139951966302368
	139951966349680 [label=PermuteBackward0]
	139951966349440 -> 139951966349680
	139951966349440 [label=UnsqueezeBackward0]
	139951966350160 -> 139951966349440
	139951966350160 [label=UnsqueezeBackward0]
	139951966350688 -> 139951966350160
	139951966350688 [label=UnsqueezeBackward0]
	139951966351120 -> 139951966350688
	139951966351120 [label=SoftmaxBackward0]
	139951966347616 -> 139951966351120
	139951966347616 [label=SumBackward1]
	139951966384288 -> 139951966347616
	139951966384288 [label=ViewBackward0]
	139951966384864 -> 139951966384288
	139951966384864 [label=AliasBackward0]
	139951966385536 -> 139951966384864
	139951966385536 [label=ReluBackward0]
	139951966385728 -> 139951966385536
	139951966385728 [label=MmBackward0]
	139951966385824 -> 139951966385728
	139951746239120 [label="adp.3.nodevec1
 (1024, 10)" fillcolor=lightblue]
	139951746239120 -> 139951966385824
	139951966385824 [label=AccumulateGrad]
	139951966385872 -> 139951966385728
	139951746239200 [label="adp.3.nodevec2
 (10, 1024)" fillcolor=lightblue]
	139951746239200 -> 139951966385872
	139951966385872 [label=AccumulateGrad]
	139951966527296 -> 139951966526912
	139952177072784 [label="up_concat3.0.conv1.weight
 (256, 1024, 3, 3)" fillcolor=lightblue]
	139952177072784 -> 139951966527296
	139951966527296 [label=AccumulateGrad]
	139951966524080 -> 139951967020128
	139952177072544 [label="up_concat3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139952177072544 -> 139951966524080
	139951966524080 [label=AccumulateGrad]
	139951967004032 -> 139951967003984
	139951967004032 [label=ViewBackward0]
	139951966564464 -> 139951967004032
	139951966564464 [label=PermuteBackward0]
	139951967019552 -> 139951966564464
	139951967019552 [label=ViewBackward0]
	139951966526480 -> 139951967019552
	139951966526480 [label=BmmBackward0]
	139951966613072 -> 139951966526480
	139951966613072 [label=UnsafeViewBackward0]
	139951966741792 -> 139951966613072
	139951966741792 [label=CloneBackward0]
	139951966644304 -> 139951966741792
	139951966644304 [label=PermuteBackward0]
	139951966350208 -> 139951966644304
	139951966350208 [label=PermuteBackward0]
	139951966350400 -> 139951966350208
	139951966350400 [label=UnsqueezeBackward0]
	139951966385008 -> 139951966350400
	139951966385008 [label=ReluBackward0]
	139951966385632 -> 139951966385008
	139951966385632 [label=ConvolutionBackward0]
	139951966385248 -> 139951966385632
	139951966385248 [label=ReluBackward0]
	139951966386016 -> 139951966385248
	139951966386016 [label=ConvolutionBackward0]
	139951966386112 -> 139951966386016
	139951966386112 [label=CatBackward0]
	139951966386256 -> 139951966386112
	139951966386256 [label=ReluBackward0]
	139951966386400 -> 139951966386256
	139951966386400 [label=NativeGroupNormBackward0]
	139951966386496 -> 139951966386400
	139951966386496 [label=ConvolutionBackward0]
	139951966386592 -> 139951966386496
	139951966386592 [label=MulBackward0]
	139951966386688 -> 139951966386592
	139951966386688 [label=CatBackward0]
	139951966565088 -> 139951966386688
	139951966565040 -> 139951966386688
	139951966564992 -> 139951966386688
	139951966386640 -> 139951966386592
	139951966386640 [label=AddBackward0]
	139951966386736 -> 139951966386640
	139951966386736 [label=TanhBackward0]
	139951966386928 -> 139951966386736
	139951966386928 [label=AddBackward0]
	139951966387024 -> 139951966386928
	139951966387024 [label=MulBackward0]
	139951966387120 -> 139951966387024
	139951966387120 [label=MulBackward0]
	139951966387264 -> 139951966387120
	139951966387264 [label=PowBackward0]
	139951966387360 -> 139951966387264
	139951966387360 [label=AddBackward0]
	139951966387456 -> 139951966387360
	139951966387456 [label=SumBackward1]
	139951966387552 -> 139951966387456
	139951966387552 [label=PowBackward0]
	139951966386688 -> 139951966387552
	139951966611680 -> 139951966387120
	139951966387072 -> 139951966387024
	139951966387072 [label=DivBackward0]
	139951966612112 -> 139951966387072
	139951966387408 -> 139951966387072
	139951966387408 [label=PowBackward0]
	139951966387648 -> 139951966387408
	139951966387648 [label=AddBackward0]
	139951966387168 -> 139951966387648
	139951966387168 [label=MeanBackward1]
	139951966387744 -> 139951966387168
	139951966387744 [label=PowBackward0]
	139951966387120 -> 139951966387744
	139951966575872 -> 139951966386928
	139951966564704 -> 139951966386496
	139951966564656 -> 139951966386496
	139951966564512 -> 139951966386400
	139951966564416 -> 139951966386400
	139951966386208 -> 139951966386112
	139951966386208 [label=UpsampleBilinear2DBackward0]
	139951966350112 -> 139951966386208
	139951966386064 -> 139951966386016
	139952177072304 [label="up_concat3.1.conv1.weight
 (256, 1024, 3, 3)" fillcolor=lightblue]
	139952177072304 -> 139951966386064
	139951966386064 [label=AccumulateGrad]
	139951966384384 -> 139951966385632
	139952177072144 [label="up_concat3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139952177072144 -> 139951966384384
	139951966384384 [label=AccumulateGrad]
	139951966526672 -> 139951966526480
	139951966526672 [label=ViewBackward0]
	139951966741024 -> 139951966526672
	139951966741024 [label=PermuteBackward0]
	139951966347328 -> 139951966741024
	139951966347328 [label=PermuteBackward0]
	139951966384192 -> 139951966347328
	139951966384192 [label=UnsqueezeBackward0]
	139951966385920 -> 139951966384192
	139951966385920 [label=UnsqueezeBackward0]
	139951966386544 -> 139951966385920
	139951966386544 [label=UnsqueezeBackward0]
	139951966386352 -> 139951966386544
	139951966386352 [label=SoftmaxBackward0]
	139951966386784 -> 139951966386352
	139951966386784 [label=SumBackward1]
	139951966386976 -> 139951966386784
	139951966386976 [label=ViewBackward0]
	139951966387312 -> 139951966386976
	139951966387312 [label=AliasBackward0]
	139951966387600 -> 139951966387312
	139951966387600 [label=ReluBackward0]
	139951966387840 -> 139951966387600
	139951966387840 [label=MmBackward0]
	139951966387216 -> 139951966387840
	139951746238960 [label="adp.2.nodevec1
 (1024, 10)" fillcolor=lightblue]
	139951746238960 -> 139951966387216
	139951966387216 [label=AccumulateGrad]
	139951966387792 -> 139951966387840
	139951746239040 [label="adp.2.nodevec2
 (10, 1024)" fillcolor=lightblue]
	139951746239040 -> 139951966387792
	139951966387792 [label=AccumulateGrad]
	139951967003312 -> 139951967003360
	139952177073504 [label="up_concat2.0.conv1.weight
 (128, 512, 3, 3)" fillcolor=lightblue]
	139952177073504 -> 139951967003312
	139951967003312 [label=AccumulateGrad]
	139951967002688 -> 139951966994192
	139952177162576 [label="up_concat2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139952177162576 -> 139951967002688
	139951967002688 [label=AccumulateGrad]
	139951966992848 -> 139951966992800
	139951966992848 [label=ViewBackward0]
	139951966993136 -> 139951966992848
	139951966993136 [label=PermuteBackward0]
	139951967003216 -> 139951966993136
	139951967003216 [label=ViewBackward0]
	139951967003408 -> 139951967003216
	139951967003408 [label=BmmBackward0]
	139951966564608 -> 139951967003408
	139951966564608 [label=UnsafeViewBackward0]
	139951966642288 -> 139951966564608
	139951966642288 [label=CloneBackward0]
	139951966348288 -> 139951966642288
	139951966348288 [label=PermuteBackward0]
	139951966385968 -> 139951966348288
	139951966385968 [label=PermuteBackward0]
	139951966386304 -> 139951966385968
	139951966386304 [label=UnsqueezeBackward0]
	139951966386832 -> 139951966386304
	139951966386832 [label=ReluBackward0]
	139951966387696 -> 139951966386832
	139951966387696 [label=ConvolutionBackward0]
	139951966387888 -> 139951966387696
	139951966387888 [label=ReluBackward0]
	139951966388032 -> 139951966387888
	139951966388032 [label=ConvolutionBackward0]
	139951966388128 -> 139951966388032
	139951966388128 [label=CatBackward0]
	139951966388176 -> 139951966388128
	139951966388176 [label=ReluBackward0]
	139951966417152 -> 139951966388176
	139951966417152 [label=NativeGroupNormBackward0]
	139951966417248 -> 139951966417152
	139951966417248 [label=ConvolutionBackward0]
	139951966417344 -> 139951966417248
	139951966417344 [label=MulBackward0]
	139951966417440 -> 139951966417344
	139951966417440 [label=CatBackward0]
	139951967004272 -> 139951966417440
	139951967004224 -> 139951966417440
	139951967004176 -> 139951966417440
	139951966417392 -> 139951966417344
	139951966417392 [label=AddBackward0]
	139951966417488 -> 139951966417392
	139951966417488 [label=TanhBackward0]
	139951966417680 -> 139951966417488
	139951966417680 [label=AddBackward0]
	139951966417776 -> 139951966417680
	139951966417776 [label=MulBackward0]
	139951966417872 -> 139951966417776
	139951966417872 [label=MulBackward0]
	139951966418016 -> 139951966417872
	139951966418016 [label=PowBackward0]
	139951966418112 -> 139951966418016
	139951966418112 [label=AddBackward0]
	139951966418208 -> 139951966418112
	139951966418208 [label=SumBackward1]
	139951966418304 -> 139951966418208
	139951966418304 [label=PowBackward0]
	139951966417440 -> 139951966418304
	139951966524848 -> 139951966417872
	139951966417824 -> 139951966417776
	139951966417824 [label=DivBackward0]
	139951966525280 -> 139951966417824
	139951966418160 -> 139951966417824
	139951966418160 [label=PowBackward0]
	139951966418400 -> 139951966418160
	139951966418400 [label=AddBackward0]
	139951966417920 -> 139951966418400
	139951966417920 [label=MeanBackward1]
	139951966418496 -> 139951966417920
	139951966418496 [label=PowBackward0]
	139951966417872 -> 139951966418496
	139951967022336 -> 139951966417680
	139951967003888 -> 139951966417248
	139951967003840 -> 139951966417248
	139951967003696 -> 139951966417152
	139951967003552 -> 139951966417152
	139951966417008 -> 139951966388128
	139951966417008 [label=UpsampleBilinear2DBackward0]
	139951966385008 -> 139951966417008
	139951966388080 -> 139951966388032
	139952177162256 [label="up_concat2.1.conv1.weight
 (128, 512, 3, 3)" fillcolor=lightblue]
	139952177162256 -> 139951966388080
	139951966388080 [label=AccumulateGrad]
	139951966384912 -> 139951966387696
	139952177161936 [label="up_concat2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139952177161936 -> 139951966384912
	139951966384912 [label=AccumulateGrad]
	139951967003120 -> 139951967003408
	139951967003120 [label=ViewBackward0]
	139951966350544 -> 139951967003120
	139951966350544 [label=PermuteBackward0]
	139951966386880 -> 139951966350544
	139951966386880 [label=PermuteBackward0]
	139951966385344 -> 139951966386880
	139951966385344 [label=UnsqueezeBackward0]
	139951966387984 -> 139951966385344
	139951966387984 [label=UnsqueezeBackward0]
	139951966386448 -> 139951966387984
	139951966386448 [label=UnsqueezeBackward0]
	139951966417104 -> 139951966386448
	139951966417104 [label=SoftmaxBackward0]
	139951966417536 -> 139951966417104
	139951966417536 [label=SumBackward1]
	139951966417728 -> 139951966417536
	139951966417728 [label=ViewBackward0]
	139951966418064 -> 139951966417728
	139951966418064 [label=AliasBackward0]
	139951966418352 -> 139951966418064
	139951966418352 [label=ReluBackward0]
	139951966418592 -> 139951966418352
	139951966418592 [label=MmBackward0]
	139951966417968 -> 139951966418592
	139951746238800 [label="adp.1.nodevec1
 (1024, 10)" fillcolor=lightblue]
	139951746238800 -> 139951966417968
	139951966417968 [label=AccumulateGrad]
	139951966418544 -> 139951966418592
	139951746238880 [label="adp.1.nodevec2
 (10, 1024)" fillcolor=lightblue]
	139951746238880 -> 139951966418544
	139951966418544 [label=AccumulateGrad]
	139951966992128 -> 139951966992080
	139952177161456 [label="up_concat1.0.conv1.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	139952177161456 -> 139951966992128
	139951966992128 [label=AccumulateGrad]
	139951966991888 -> 139951966991840
	139952177161296 [label="up_concat1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139952177161296 -> 139951966991888
	139951966991888 [label=AccumulateGrad]
	139951966991648 -> 139951966991600
	139951966991648 [label=ViewBackward0]
	139951966992416 -> 139951966991648
	139951966992416 [label=PermuteBackward0]
	139951966992032 -> 139951966992416
	139951966992032 [label=ViewBackward0]
	139951966992224 -> 139951966992032
	139951966992224 [label=BmmBackward0]
	139951966991792 -> 139951966992224
	139951966991792 [label=UnsafeViewBackward0]
	139951966524512 -> 139951966991792
	139951966524512 [label=CloneBackward0]
	139951967003792 -> 139951966524512
	139951967003792 [label=PermuteBackward0]
	139951966387936 -> 139951967003792
	139951966387936 [label=PermuteBackward0]
	139951966417056 -> 139951966387936
	139951966417056 [label=UnsqueezeBackward0]
	139951966417584 -> 139951966417056
	139951966417584 [label=ReluBackward0]
	139951966418448 -> 139951966417584
	139951966418448 [label=ConvolutionBackward0]
	139951966418640 -> 139951966418448
	139951966418640 [label=ReluBackward0]
	139951966418784 -> 139951966418640
	139951966418784 [label=ConvolutionBackward0]
	139951966418880 -> 139951966418784
	139951966418880 [label=CatBackward0]
	139951966419024 -> 139951966418880
	139951966419024 [label=ReluBackward0]
	139951966419168 -> 139951966419024
	139951966419168 [label=NativeGroupNormBackward0]
	139951966419264 -> 139951966419168
	139951966419264 [label=ConvolutionBackward0]
	139951966419360 -> 139951966419264
	139951966419360 [label=MulBackward0]
	139951966419456 -> 139951966419360
	139951966419456 [label=CatBackward0]
	139951966993088 -> 139951966419456
	139951966993040 -> 139951966419456
	139951966992992 -> 139951966419456
	139951966419408 -> 139951966419360
	139951966419408 [label=AddBackward0]
	139951966419504 -> 139951966419408
	139951966419504 [label=TanhBackward0]
	139951966419696 -> 139951966419504
	139951966419696 [label=AddBackward0]
	139951966419792 -> 139951966419696
	139951966419792 [label=MulBackward0]
	139951966419888 -> 139951966419792
	139951966419888 [label=MulBackward0]
	139951966420032 -> 139951966419888
	139951966420032 [label=PowBackward0]
	139951966420128 -> 139951966420032
	139951966420128 [label=AddBackward0]
	139951966420224 -> 139951966420128
	139951966420224 [label=SumBackward1]
	139951966420320 -> 139951966420224
	139951966420320 [label=PowBackward0]
	139951966419456 -> 139951966420320
	139951966994336 -> 139951966419888
	139951966419840 -> 139951966419792
	139951966419840 [label=DivBackward0]
	139951967002880 -> 139951966419840
	139951966420176 -> 139951966419840
	139951966420176 [label=PowBackward0]
	139951966420416 -> 139951966420176
	139951966420416 [label=AddBackward0]
	139951966419936 -> 139951966420416
	139951966419936 [label=MeanBackward1]
	139951966420512 -> 139951966419936
	139951966420512 [label=PowBackward0]
	139951966419888 -> 139951966420512
	139951966994096 -> 139951966419696
	139951966992704 -> 139951966419264
	139951966992656 -> 139951966419264
	139951966992512 -> 139951966419168
	139951966992368 -> 139951966419168
	139951966418976 -> 139951966418880
	139951966418976 [label=UpsampleBilinear2DBackward0]
	139951966386832 -> 139951966418976
	139951966418832 -> 139951966418784
	139952177160976 [label="up_concat1.1.conv1.weight
 (64, 192, 3, 3)" fillcolor=lightblue]
	139952177160976 -> 139951966418832
	139951966418832 [label=AccumulateGrad]
	139951966417200 -> 139951966418448
	139952177160656 [label="up_concat1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139952177160656 -> 139951966417200
	139951966417200 [label=AccumulateGrad]
	139951967002736 -> 139951966992224
	139951967002736 [label=ViewBackward0]
	139951966386160 -> 139951967002736
	139951966386160 [label=PermuteBackward0]
	139951966525904 -> 139951966386160
	139951966525904 [label=PermuteBackward0]
	139951966417296 -> 139951966525904
	139951966417296 [label=UnsqueezeBackward0]
	139951966418688 -> 139951966417296
	139951966418688 [label=UnsqueezeBackward0]
	139951966419312 -> 139951966418688
	139951966419312 [label=UnsqueezeBackward0]
	139951966419120 -> 139951966419312
	139951966419120 [label=SoftmaxBackward0]
	139951966419552 -> 139951966419120
	139951966419552 [label=SumBackward1]
	139951966419744 -> 139951966419552
	139951966419744 [label=ViewBackward0]
	139951966420080 -> 139951966419744
	139951966420080 [label=AliasBackward0]
	139951966420368 -> 139951966420080
	139951966420368 [label=ReluBackward0]
	139951966420608 -> 139951966420368
	139951966420608 [label=MmBackward0]
	139951966419984 -> 139951966420608
	139951746238320 [label="adp.0.nodevec1
 (1024, 10)" fillcolor=lightblue]
	139951746238320 -> 139951966419984
	139951966419984 [label=AccumulateGrad]
	139951966420560 -> 139951966420608
	139951746238720 [label="adp.0.nodevec2
 (10, 1024)" fillcolor=lightblue]
	139951746238720 -> 139951966420560
	139951966420560 [label=AccumulateGrad]
	139951966991360 -> 139951966991312
	139952177160336 [label="up_conv.0.1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139952177160336 -> 139951966991360
	139951966991360 [label=AccumulateGrad]
	139951966991216 -> 139951966991312
	139952177162896 [label="up_conv.0.1.bias
 (64)" fillcolor=lightblue]
	139952177162896 -> 139951966991216
	139951966991216 [label=AccumulateGrad]
	139951966991072 -> 139951966991024
	139952177141376 [label="up_conv.0.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139952177141376 -> 139951966991072
	139951966991072 [label=AccumulateGrad]
	139951966990928 -> 139951966991024
	139952177141136 [label="up_conv.0.3.bias
 (64)" fillcolor=lightblue]
	139952177141136 -> 139951966990928
	139951966990928 [label=AccumulateGrad]
	139951966990784 -> 139951966990736
	139951746239360 [label="final.0.0.weight
 (64)" fillcolor=lightblue]
	139951746239360 -> 139951966990784
	139951966990784 [label=AccumulateGrad]
	139951966990400 -> 139951966990736
	139951746239440 [label="final.0.0.bias
 (64)" fillcolor=lightblue]
	139951746239440 -> 139951966990400
	139951966990400 [label=AccumulateGrad]
	139951966990640 -> 139951966990592
	139951746239600 [label="final.0.2.weight
 (8, 64, 3, 3)" fillcolor=lightblue]
	139951746239600 -> 139951966990640
	139951966990640 [label=AccumulateGrad]
	139951966990544 -> 139951966990592
	139951746239680 [label="final.0.2.bias
 (8)" fillcolor=lightblue]
	139951746239680 -> 139951966990544
	139951966990544 [label=AccumulateGrad]
	139951966990592 -> 139952119481664
	139952119482544 [label="
 (4, 6, 384, 512)" fillcolor=darkolivegreen1]
	139951966990496 [label=ConvolutionBackward0]
	139951966990976 -> 139951966990496
	139951966990976 [label=ReluBackward0]
	139951966991552 -> 139951966990976
	139951966991552 [label=NativeGroupNormBackward0]
	139951966991744 -> 139951966991552
	139951966991744 [label=ReluBackward0]
	139951966387504 -> 139951966991744
	139951966387504 [label=ConvolutionBackward0]
	139951966992608 -> 139951966387504
	139951966992608 [label=ReluBackward0]
	139951966419072 -> 139951966992608
	139951966419072 [label=ConvolutionBackward0]
	139951966419600 -> 139951966419072
	139951966419600 [label=UpsampleBilinear2DBackward0]
	139951966420656 -> 139951966419600
	139951966420656 [label=AddBackward0]
	139951966420752 -> 139951966420656
	139951966420752 [label=ReluBackward0]
	139951966420896 -> 139951966420752
	139951966420896 [label=ConvolutionBackward0]
	139951966420944 -> 139951966420896
	139951966420944 [label=ReluBackward0]
	139951966445728 -> 139951966420944
	139951966445728 [label=ConvolutionBackward0]
	139951966445824 -> 139951966445728
	139951966445824 [label=CatBackward0]
	139951966419024 -> 139951966445824
	139951966445920 -> 139951966445824
	139951966445920 [label=UpsampleBilinear2DBackward0]
	139951966446016 -> 139951966445920
	139951966446016 [label=AddBackward0]
	139951966446112 -> 139951966446016
	139951966446112 [label=ReluBackward0]
	139951966446256 -> 139951966446112
	139951966446256 [label=ConvolutionBackward0]
	139951966446352 -> 139951966446256
	139951966446352 [label=ReluBackward0]
	139951966446448 -> 139951966446352
	139951966446448 [label=ConvolutionBackward0]
	139951966446544 -> 139951966446448
	139951966446544 [label=CatBackward0]
	139951966388176 -> 139951966446544
	139951966446640 -> 139951966446544
	139951966446640 [label=UpsampleBilinear2DBackward0]
	139951966446736 -> 139951966446640
	139951966446736 [label=AddBackward0]
	139951966446832 -> 139951966446736
	139951966446832 [label=ReluBackward0]
	139951966446976 -> 139951966446832
	139951966446976 [label=ConvolutionBackward0]
	139951966447072 -> 139951966446976
	139951966447072 [label=ReluBackward0]
	139951966447168 -> 139951966447072
	139951966447168 [label=ConvolutionBackward0]
	139951966447264 -> 139951966447168
	139951966447264 [label=CatBackward0]
	139951966386256 -> 139951966447264
	139951966447360 -> 139951966447264
	139951966447360 [label=UpsampleBilinear2DBackward0]
	139951966447456 -> 139951966447360
	139951966447456 [label=AddBackward0]
	139951966350112 -> 139951966447456
	139951966447552 -> 139951966447456
	139951966447552 [label=ViewBackward0]
	139951966447648 -> 139951966447552
	139951966447648 [label=PermuteBackward0]
	139951966447744 -> 139951966447648
	139951966447744 [label=ViewBackward0]
	139951966447840 -> 139951966447744
	139951966447840 [label=BmmBackward0]
	139951966447936 -> 139951966447840
	139951966447936 [label=UnsafeViewBackward0]
	139951966448080 -> 139951966447936
	139951966448080 [label=CloneBackward0]
	139951966448176 -> 139951966448080
	139951966448176 [label=PermuteBackward0]
	139951966448272 -> 139951966448176
	139951966448272 [label=PermuteBackward0]
	139951966448368 -> 139951966448272
	139951966448368 [label=UnsqueezeBackward0]
	139951966565232 -> 139951966448368
	139951966447888 -> 139951966447840
	139951966447888 [label=ViewBackward0]
	139951966448224 -> 139951966447888
	139951966448224 [label=PermuteBackward0]
	139951966448464 -> 139951966448224
	139951966448464 [label=PermuteBackward0]
	139951966447984 -> 139951966448464
	139951966447984 [label=UnsqueezeBackward0]
	139951966448560 -> 139951966447984
	139951966448560 [label=UnsqueezeBackward0]
	139951966448656 -> 139951966448560
	139951966448656 [label=UnsqueezeBackward0]
	139951966351120 -> 139951966448656
	139951966386064 -> 139951966447168
	139951966384384 -> 139951966446976
	139951966446784 -> 139951966446736
	139951966446784 [label=ViewBackward0]
	139951966447120 -> 139951966446784
	139951966447120 [label=PermuteBackward0]
	139951966447312 -> 139951966447120
	139951966447312 [label=ViewBackward0]
	139951966447504 -> 139951966447312
	139951966447504 [label=BmmBackward0]
	139951966447696 -> 139951966447504
	139951966447696 [label=UnsafeViewBackward0]
	139951966448128 -> 139951966447696
	139951966448128 [label=CloneBackward0]
	139951966448416 -> 139951966448128
	139951966448416 [label=PermuteBackward0]
	139951966448608 -> 139951966448416
	139951966448608 [label=PermuteBackward0]
	139951966448704 -> 139951966448608
	139951966448704 [label=UnsqueezeBackward0]
	139951966448800 -> 139951966448704
	139951966448800 [label=ReluBackward0]
	139951966448896 -> 139951966448800
	139951966448896 [label=ConvolutionBackward0]
	139951966448992 -> 139951966448896
	139951966448992 [label=ReluBackward0]
	139951966449088 -> 139951966448992
	139951966449088 [label=ConvolutionBackward0]
	139951966449184 -> 139951966449088
	139951966449184 [label=CatBackward0]
	139951966527392 -> 139951966449184
	139951966449280 -> 139951966449184
	139951966449280 [label=UpsampleBilinear2DBackward0]
	139951966565232 -> 139951966449280
	139951966527296 -> 139951966449088
	139951966524080 -> 139951966448896
	139951966447600 -> 139951966447504
	139951966447600 [label=ViewBackward0]
	139951966448512 -> 139951966447600
	139951966448512 [label=PermuteBackward0]
	139951966448032 -> 139951966448512
	139951966448032 [label=PermuteBackward0]
	139951966448944 -> 139951966448032
	139951966448944 [label=UnsqueezeBackward0]
	139951966449136 -> 139951966448944
	139951966449136 [label=UnsqueezeBackward0]
	139951966449376 -> 139951966449136
	139951966449376 [label=UnsqueezeBackward0]
	139951966386352 -> 139951966449376
	139951966388080 -> 139951966446448
	139951966384912 -> 139951966446256
	139951966446064 -> 139951966446016
	139951966446064 [label=ViewBackward0]
	139951966446400 -> 139951966446064
	139951966446400 [label=PermuteBackward0]
	139951966446592 -> 139951966446400
	139951966446592 [label=ViewBackward0]
	139951966446160 -> 139951966446592
	139951966446160 [label=BmmBackward0]
	139951966447216 -> 139951966446160
	139951966447216 [label=UnsafeViewBackward0]
	139951966448320 -> 139951966447216
	139951966448320 [label=CloneBackward0]
	139951966448848 -> 139951966448320
	139951966448848 [label=PermuteBackward0]
	139951966449232 -> 139951966448848
	139951966449232 [label=PermuteBackward0]
	139951966449328 -> 139951966449232
	139951966449328 [label=UnsqueezeBackward0]
	139951966449424 -> 139951966449328
	139951966449424 [label=ReluBackward0]
	139951966449520 -> 139951966449424
	139951966449520 [label=ConvolutionBackward0]
	139951966449616 -> 139951966449520
	139951966449616 [label=ReluBackward0]
	139951966447408 -> 139951966449616
	139951966447408 [label=ConvolutionBackward0]
	139951966462160 -> 139951966447408
	139951966462160 [label=CatBackward0]
	139951967003504 -> 139951966462160
	139951966462256 -> 139951966462160
	139951966462256 [label=UpsampleBilinear2DBackward0]
	139951966448800 -> 139951966462256
	139951967003312 -> 139951966447408
	139951967002688 -> 139951966449520
	139951966447024 -> 139951966446160
	139951966447024 [label=ViewBackward0]
	139951966449040 -> 139951966447024
	139951966449040 [label=PermuteBackward0]
	139951966446880 -> 139951966449040
	139951966446880 [label=PermuteBackward0]
	139951966449568 -> 139951966446880
	139951966449568 [label=UnsqueezeBackward0]
	139951966446928 -> 139951966449568
	139951966446928 [label=UnsqueezeBackward0]
	139951966462352 -> 139951966446928
	139951966462352 [label=UnsqueezeBackward0]
	139951966417104 -> 139951966462352
	139951966418832 -> 139951966445728
	139951966417200 -> 139951966420896
	139951966420704 -> 139951966420656
	139951966420704 [label=ViewBackward0]
	139951966420800 -> 139951966420704
	139951966420800 [label=PermuteBackward0]
	139951966445872 -> 139951966420800
	139951966445872 [label=ViewBackward0]
	139951966445632 -> 139951966445872
	139951966445632 [label=BmmBackward0]
	139951966446496 -> 139951966445632
	139951966446496 [label=UnsafeViewBackward0]
	139951966448752 -> 139951966446496
	139951966448752 [label=CloneBackward0]
	139951966449472 -> 139951966448752
	139951966449472 [label=PermuteBackward0]
	139951966446688 -> 139951966449472
	139951966446688 [label=PermuteBackward0]
	139951966462304 -> 139951966446688
	139951966462304 [label=UnsqueezeBackward0]
	139951966462400 -> 139951966462304
	139951966462400 [label=ReluBackward0]
	139951966462496 -> 139951966462400
	139951966462496 [label=ConvolutionBackward0]
	139951966462592 -> 139951966462496
	139951966462592 [label=ReluBackward0]
	139951966462688 -> 139951966462592
	139951966462688 [label=ConvolutionBackward0]
	139951966462784 -> 139951966462688
	139951966462784 [label=CatBackward0]
	139951966992320 -> 139951966462784
	139951966462880 -> 139951966462784
	139951966462880 [label=UpsampleBilinear2DBackward0]
	139951966449424 -> 139951966462880
	139951966992128 -> 139951966462688
	139951966991888 -> 139951966462496
	139951966446304 -> 139951966445632
	139951966446304 [label=ViewBackward0]
	139951966447792 -> 139951966446304
	139951966447792 [label=PermuteBackward0]
	139951966462064 -> 139951966447792
	139951966462064 [label=PermuteBackward0]
	139951966462544 -> 139951966462064
	139951966462544 [label=UnsqueezeBackward0]
	139951966462736 -> 139951966462544
	139951966462736 [label=UnsqueezeBackward0]
	139951966462976 -> 139951966462736
	139951966462976 [label=UnsqueezeBackward0]
	139951966419120 -> 139951966462976
	139951966419648 -> 139951966419072
	139952177140656 [label="up_conv.1.1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139952177140656 -> 139951966419648
	139951966419648 [label=AccumulateGrad]
	139951966418736 -> 139951966419072
	139952177140496 [label="up_conv.1.1.bias
 (64)" fillcolor=lightblue]
	139952177140496 -> 139951966418736
	139951966418736 [label=AccumulateGrad]
	139951966418256 -> 139951966387504
	139952177140016 [label="up_conv.1.3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139952177140016 -> 139951966418256
	139951966418256 [label=AccumulateGrad]
	139951966417632 -> 139951966387504
	139952177140176 [label="up_conv.1.3.bias
 (64)" fillcolor=lightblue]
	139952177140176 -> 139951966417632
	139951966417632 [label=AccumulateGrad]
	139951966991504 -> 139951966991552
	139951746239760 [label="final.1.0.weight
 (64)" fillcolor=lightblue]
	139951746239760 -> 139951966991504
	139951966991504 [label=AccumulateGrad]
	139951966991264 -> 139951966991552
	139951746239840 [label="final.1.0.bias
 (64)" fillcolor=lightblue]
	139951746239840 -> 139951966991264
	139951966991264 [label=AccumulateGrad]
	139951966990880 -> 139951966990496
	139951746240000 [label="final.1.2.weight
 (6, 64, 3, 3)" fillcolor=lightblue]
	139951746240000 -> 139951966990880
	139951966990880 [label=AccumulateGrad]
	139951966990448 -> 139951966990496
	139951746240080 [label="final.1.2.bias
 (6)" fillcolor=lightblue]
	139951746240080 -> 139951966990448
	139951966990448 [label=AccumulateGrad]
	139951966990496 -> 139952119482544
}
